<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"ropo0107.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"slideLeftIn","sidebar":"slideDownBigOut"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="无恙の博客">
<meta property="og:url" content="https://ropo0107.github.io/page/3/index.html">
<meta property="og:site_name" content="无恙の博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="无恙">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://ropo0107.github.io/page/3/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>无恙の博客</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?3740b075e17748c0bde2d912b5d8f093"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">无恙の博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">幸得识卿桃花面，从此阡陌多暖春！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="无恙"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">无恙</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ropo0107" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ropo0107" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangzongtao123@gmail.com" title="E-Mail → mailto:wangzongtao123@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="/soup/darksoup/" title="&#x2F;soup&#x2F;darksoup&#x2F;">毒鸡汤</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="/soup/rainbow/" title="&#x2F;soup&#x2F;rainbow&#x2F;">彩虹屁</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/07/19/robot/ros_workspace/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/19/robot/ros_workspace/" class="post-title-link" itemprop="url">【ros】 workspace and namespace</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-07-19 23:34:55" itemprop="dateCreated datePublished" datetime="2020-07-19T23:34:55+08:00">2020-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>一直想把之前ros的坑给补上， 最近终于有机会了</p>
<h1 id="工作空间"><a href="#工作空间" class="headerlink" title="工作空间"></a>工作空间</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>工作空间(workspace)是存放工程开发相关文件的文件夹。现ROS默认使用Catkin编译系统。<br>典型的工作空间包括四部分：</p>
<ul>
<li>src ： 放源代码文件</li>
<li>build ： 放编译中的缓存和中间文件</li>
<li>devel ： 放编译生成的可执行文件</li>
<li>install ： （非必需）</li>
</ul>
<h2 id="工作空间的相互依赖"><a href="#工作空间的相互依赖" class="headerlink" title="工作空间的相互依赖"></a>工作空间的相互依赖</h2><ul>
<li>若想查看所有ROS相关的环境变量：   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">env</span> | grep ros</span><br></pre></td></tr></table></figure></li>
<li>若想查看ROS_PACKAGE_PATH：  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span> </span><br></pre></td></tr></table></figure></li>
<li>激活相应工作空间  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据自己启用的shell工具决定， bash用sh，zsh用zsh</span></span><br><span class="line">soucrce &#123;workspace&#125;/devel/setup.sh</span><br><span class="line"></span><br><span class="line">soucrce &#123;workspace&#125;/devel/setup.zsh</span><br></pre></td></tr></table></figure></li>
<li>工作空间编译依赖问题:<ul>
<li><p>默认的工作空间,即使用 sudo apt-get install ros-kinetic-desktop-full, 安装的默认位置为:</p>
<p>  &#x2F;opt&#x2F;ros&#x2F;kinetic&#x2F;setup.zsh</p>
</li>
<li><p>激活默认环境:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /opt/ros/kinetic/setup.zsh</span><br></pre></td></tr></table></figure></li>
<li><p>查看环境变量</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">/opt/ros/kinetic/share     </span><br></pre></td></tr></table></figure></li>
<li><p>激活默认环境后激活要用的工作空间， 这儿我的是 ur_ws</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile</span></span><br><span class="line"><span class="built_in">source</span> /opt/ros/kinetic/setup.zsh</span><br><span class="line"><span class="built_in">cd</span> ~/ros/ur_ws/</span><br><span class="line">catkin_make</span><br><span class="line"></span><br><span class="line"><span class="comment"># use new workspace</span></span><br><span class="line"><span class="built_in">source</span> ~/ros/ur_ws/devel/setup.zsh</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output                         </span></span><br><span class="line">/home/sunshine/ros/ur_ws/src:/opt/ros/kinetic/share</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  新的workspace在前面，被依赖的workspace在后面 如果两个公共空间有重名的功能包，调用新的工作空间的功能包。</p>
</li>
<li><p>这儿有个问题，如果接下来要用的工作空间arm_ws继续依赖前面两个工作空间呢</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile</span></span><br><span class="line"><span class="built_in">source</span> <span class="built_in">source</span> ~/ros/ur_ws/devel/setup.zsh</span><br><span class="line"><span class="built_in">cd</span> ~/ros/arm_ws/</span><br><span class="line">catkin_make</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output                         </span></span><br><span class="line">/home/user/ros/arm_ws/src:/home/user/ros/ur_ws/src:/opt/ros/kinetic/share</span><br><span class="line"></span><br><span class="line"><span class="comment"># use arm_ws</span></span><br><span class="line"><span class="built_in">source</span> <span class="built_in">source</span> ~/ros/arm_ws/devel/setup.zsh</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul>
<li><p>工作空间的依赖只在编译时有用，</p>
<ul>
<li><strong>c++:</strong> 因为大部分程序采样 <strong>c++</strong> 编写， c++为编译型语言， 故修改c++程序后需要重新编译工作空间</li>
<li><strong>python:</strong> 为解释型语言，如果修改部分为python语言，则不需要重新编译工作空间</li>
</ul>
</li>
<li><p>在使用相应工作空间的时候，需要用source激活，可以用alias来简化</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ros=<span class="string">&#x27;source /opt/ros/kinetic/setup.zsh&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="namespace"><a href="#namespace" class="headerlink" title="namespace"></a>namespace</h1><p>在实际的ROS网络中，各个节点、话题、消息和参数的名称必须是唯一的，不然就会发生冲突。</p>
<p>因此最简单的两种方法，来区分相同名字的资源：</p>
<ul>
<li>给两个名字前加上定语，就是 <strong>添加命名空间</strong>；</li>
<li>给两个名字取个不同的别名，就是 <strong>重映射</strong>；</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/07/11/robot/quaternion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/11/robot/quaternion/" class="post-title-link" itemprop="url">【robot】 quaternion</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-07-11 23:34:55" itemprop="dateCreated datePublished" datetime="2020-07-11T23:34:55+08:00">2020-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>对于坐标的变换，旋转和平移是基本的两步，平移可以是一个标量， 旋转有多种形式可以表示，（RPY， 欧拉角， 四元数， 旋转矢量， 旋转矩阵）</p>
<h2 id="RPY"><a href="#RPY" class="headerlink" title="RPY"></a>RPY</h2><p>固定（参考）坐标系 A 旋转：假设开始两个坐标系重合，先将 B 绕 A 的 X 轴旋转$\alpha$，然后绕 A 的 Y 轴旋转 $\beta$，最后绕 A 的 Z 轴旋转 $\gamma$，就能旋转到当前姿态。可以称其(Roll, Pitch, Yaw)。由于是绕固定坐标系旋转，则旋转矩阵为：<br>$$R_{XYZ}​(\alpha,\beta,\gamma)&#x3D;R_Z​(\gamma)R_Y​(\beta)R_X(\alpha)$$</p>
<h2 id="欧拉角"><a href="#欧拉角" class="headerlink" title="欧拉角"></a>欧拉角</h2><p><img src="/images/posts/robot/quaternion/eular.gif"></p>
<p>图中演示的就是用 $(z, x,z^{‘})$ 方法表示方向的过程。在此过程中，有如下图展示的 $(\alpha, \beta,\gamma)$三个角，分别绕着原坐标z轴(蓝)，一次旋转以后的x轴(绿)以及两次旋转以后的z轴(红)旋转，最终产生的红色坐标系即表示出目标方向。</p>
<p>绕自身坐标轴 B 旋转：假设开始两个坐标系重合，先将 B 绕自身的 Z 轴旋转 $\gamma$，然后绕 Y 轴旋转 $\beta$，最后绕 X 轴旋转 $\alpha$，就能旋转到当前姿态。称其为Z-Y-X欧拉角，由于是绕自身坐标轴进行旋转，则旋转矩阵为:<br>$$R_{X^{‘}Y^{‘}Z^{‘}}​(\alpha,\beta,\gamma)&#x3D;R_{Z^{‘}}​(\gamma)R_{Y^{‘}}​(\beta)R_{X^{‘}}(\alpha)$$</p>
<h2 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h2><p>  坐标系B由坐标系A旋转得到，$^A_BR$为坐标系B的各轴单位向量在坐标系A中的投影<br>  $$^A_BR &#x3D; \begin{bmatrix} ^A\hat{X}_B &amp; ^A\hat{X}_B &amp; ^A\hat{X}_B  \end{bmatrix}$$</p>
<h2 id="旋转矩阵的左乘和右乘"><a href="#旋转矩阵的左乘和右乘" class="headerlink" title="旋转矩阵的左乘和右乘"></a>旋转矩阵的左乘和右乘</h2><p>  如上面 RPY 和欧拉角绕轴旋转得到 $R_Z$、$R_Y$、$R_Z$乘法顺序。其中rpy绕定轴旋转是左乘，欧拉角绕自身坐标系即动轴旋转是右乘</p>
<ul>
<li>这里的左右乘指的并不是某一向量左乘或者右乘旋转矩阵，而是多个旋转矩阵的组合方式是左乘还是右乘。</li>
<li>这句话还遗漏了一个相当重要的信息，绕固定坐标系旋转讨论的是向量的旋转，绕自身坐标系旋转讨论的是坐标变换！这是完全不一样的两种功能，</li>
</ul>
<h2 id="齐次变换矩阵的描述"><a href="#齐次变换矩阵的描述" class="headerlink" title="齐次变换矩阵的描述"></a>齐次变换矩阵的描述</h2><ul>
<li><p>坐标系的描述<br>$_B^A{T}$表示$B$坐标系基于$A$坐标系的变换， $<em>B^A{R}$的各列是坐标系$B$主轴方向上的单位矢量，$^A{P}</em>{BORG}$确定$B$的原点。</p>
</li>
<li><p>变换映射（同一点相对于不同坐标系的描述）<br>$$^A{P} &#x3D; _B^A{T} ^B{P}$$</p>
<p>$$\begin{bmatrix} ^A{P}\1\end{bmatrix} &#x3D;\begin{bmatrix} ^A_B{R} &amp; ^A{P}_{BORG} \ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} ^B{P}\1\end{bmatrix}$$</p>
</li>
<li><p>变换算子（同一坐标系下，向量的旋转）<br>$$^A{P}_2 &#x3D; {T} ^A{P}_1$$<br>一个矢量在当前坐标系下的变化, 其中$T$为1到2的变化, 其中$T$可看作 $_2^1{T}$。</p>
</li>
</ul>
<h2 id="RPY和欧拉角的联系"><a href="#RPY和欧拉角的联系" class="headerlink" title="RPY和欧拉角的联系"></a>RPY和欧拉角的联系</h2><p>经过RPY旋转（定轴，外旋）可以到达另一个坐标系，等价于经过绕动轴ZYX旋转（内旋）到达另一个坐标系<br>$$Eular_{zyx} &#x3D; (yaw, pitch, roll)$$</p>
<h2 id="四元数定义"><a href="#四元数定义" class="headerlink" title="四元数定义"></a>四元数定义</h2><p>首先，定义一个你需要做的旋转。旋转轴为向量$v&#x3D;(vx,vy,vz)$，旋转角度为$\theta$（右手法则的旋转）</p>
<p><img src="/images/posts/robot/quaternion/quaternion.jpg"></p>
<p>图中$v&#x3D;(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}}, \frac{3}{\sqrt{14}}), \theta &#x3D; \frac{\pi}{3}$</p>
<p>那么与此相对应的四元数（下三行式子都是一个意思，只是不同的表达形式）</p>
<ul>
<li>$$q&#x3D;(\cos(\frac{\theta}{2}), \sin(\frac{\theta}{2})*vx,\sin(\frac{\theta}{2})*vy,\sin(\frac{\theta}{2})*vz$$</li>
<li>$$q&#x3D;(\cos(\frac{\pi}{6}), \sin(\frac{\pi}{6})<em>\frac{1}{\sqrt{14}},sin(\frac{\pi}{6})</em>\frac{2}{\sqrt{14}}, \sin(\frac{\pi}{6})*\frac{3}{\sqrt{14}})$$</li>
<li>$$q&#x3D;(\cos(\frac{\pi}{6}) + \sin(\frac{\pi}{6})<em>\frac{1}{\sqrt{14}} i + sin(\frac{\pi}{6})</em>\frac{2}{\sqrt{14}} j +  \sin(\frac{\pi}{6})*\frac{3}{\sqrt{14}}k)$$</li>
</ul>
<p>它的共轭</p>
<ul>
<li>$$q^{-1}&#x3D;(\cos(\frac{\theta}{2}), -\sin(\frac{\theta}{2})*vx,-\sin(\frac{\theta}{2})*vy,-\sin(\frac{\theta}{2})*vz$$</li>
<li>$$q^{-1}&#x3D;(\cos(\frac{\pi}{6}), -\sin(\frac{\pi}{6})<em>\frac{1}{\sqrt{14}}, - sin(\frac{\pi}{6})</em>\frac{2}{\sqrt{14}}, -\sin(\frac{\pi}{6})*\frac{3}{\sqrt{14}})$$</li>
<li>$$q^{-1}&#x3D;(\cos(\frac{\pi}{6}) - \sin(\frac{\pi}{6})<em>\frac{1}{\sqrt{14}} i - sin(\frac{\pi}{6})</em>\frac{2}{\sqrt{14}} j - \sin(\frac{\pi}{6})*\frac{3}{\sqrt{14}}k)$$</li>
</ul>
<h2 id="旋转矢量和四元数的联系"><a href="#旋转矢量和四元数的联系" class="headerlink" title="旋转矢量和四元数的联系"></a>旋转矢量和四元数的联系</h2><p>四元数由4个值组成， 可以通过旋转矢量中的3个量表示。方法是通过定轴旋转可得到四元数中的<br>轴$v&#x3D;(vx,vy,vz)$以及角$\theta$。<br>$$RotateVector &#x3D; \theta<em>v&#x3D;\theta</em>(vx,vy,vz)$$</p>
<h2 id="四元数运算"><a href="#四元数运算" class="headerlink" title="四元数运算"></a>四元数运算</h2><p>求$w&#x3D;(wx,wy,wz)$在旋转下的新的坐标$w’$</p>
<ul>
<li>定义纯四元数<br>$$qw&#x3D;(0,wx,wy,wz) &#x3D;0+ wx<em>i+ wy</em>j+wz*k$$</li>
<li>进行四元数运算<br>$$qw’&#x3D;q<em>qw</em>q^{-1}$$</li>
<li>产生的$qw’$一定是四元数，即第一项为0<br>$$qw‘&#x3D;(0,wx‘,wy’,wz‘) &#x3D;0+ wx’*i+ wy‘*j+wz’*k$$</li>
<li>$qw’$的后三项$(wx’,wt’,wz’)$就是$w’$<br>$$w’&#x3D;(wx’,wy’,wx’)$$</li>
</ul>
<p>这样就完成了四元数的一次转换</p>
<h2 id="四元数求夹角"><a href="#四元数求夹角" class="headerlink" title="四元数求夹角"></a>四元数求夹角</h2><p>已知两个四元数$q_0$和$q_1$, 求两个相对旋转的夹角</p>
<ul>
<li><p>假设已知$\Delta q$:</p>
<p>  $\Delta q q_0 &#x3D; q_1$</p>
<p>  $\Delta q q_0 q_0^{-1}&#x3D; q_1 q_0^{-1}$</p>
<p>  $\Delta q &#x3D; q_1 q_0^{-1}$</p>
</li>
<li><p>假设两个四元数为单位四元数 $q^{-1} &#x3D; q^*$，</p>
<p>  $\Delta q&#x3D;q_1 q_0^{*}$</p>
</li>
<li><p>一个单位四元数的 $t$ 次幂等同于将它的旋转角度缩放至 $t$ 倍，并且不会改变它的旋转轴</p>
</li>
</ul>
<h2 id="3D空间旋转变化量-vs-四元数4D向量空间夹角"><a href="#3D空间旋转变化量-vs-四元数4D向量空间夹角" class="headerlink" title="3D空间旋转变化量 vs 四元数4D向量空间夹角"></a>3D空间旋转变化量 vs 四元数4D向量空间夹角</h2><ul>
<li><p>四元数作为四维空间中的向量，表示为<br>$$q_0&#x3D;\begin{bmatrix}<br>\cos(\theta_0) \<br>w_{x0}\sin(\theta_0) \<br>w_{y0}\sin(\theta_0) \<br>w_{z0}\sin(\theta_0) \<br>\end{bmatrix}， </p>
<p>q_1&#x3D;\begin{bmatrix}<br>\cos(\theta_1) \<br>w_{x1}\sin(\theta_1) \<br>w_{y1}\sin(\theta_1) \<br>w_{z1}\sin(\theta_1) \<br>\end{bmatrix}$$</p>
</li>
<li><p>根据向量点乘关系$q_0 \cdot q_1 &#x3D;|q_0|\cdot |q_1|\cos(\theta)$, 假定单位四元数，<br>  $$q_0 \cdot q_1 &#x3D;\cos(\theta)$$</p>
</li>
<li><p><strong>NOTE</strong> 上面的东西正好为$\Delta q$ 的实部</p>
<p>这也就是说，$q_0$ 与 $q_1$ 作为向量在 4D 四元数空间中的夹角 $\theta$，正好被它们旋转变化量 $∆q$ 所平分，四元数定义</p>
<p><strong>$\Delta q$ 可作为$q_0$ 与 $q_1$ 的中间插值</strong></p>
</li>
</ul>
<h2 id="理解四元数"><a href="#理解四元数" class="headerlink" title="理解四元数"></a>理解四元数</h2><ul>
<li>性质<ul>
<li>运算产生的结果也要是三维向量</li>
<li>存在一个元运算，任何三维向量进行元运算的结果就是其本身</li>
<li>对于任何一个运算，都存在一个逆运算，这两个运算的积是元运算</li>
<li>运算满足结合律</li>
</ul>
</li>
<li>解释<ul>
<li>四元数不是简单的映射关系，而是一个群！（后来我们知道四元数所在群为S3，而四元数所代表的三维旋转是SO(3)，前者是后者的两倍覆盖</li>
<li>为什么公式里面是$\frac{\theta}{2}$而不是$\theta$,因为$w$不为零,结果就不与我们最初的点在同一三维空间。为了解决这个问题，所以先只旋转一半，再用逆反过来旋转另一半，这样正好将第一次旋转产生的第四维变为0，这样就能正确地映射到三维超平面。<br> <img src="/images/posts/robot/quaternion/three_dim.jpg"></li>
</ul>
</li>
</ul>
<h2 id="screw"><a href="#screw" class="headerlink" title="screw"></a>screw</h2><h2 id="对偶四元数-Dual-Quaternion"><a href="#对偶四元数-Dual-Quaternion" class="headerlink" title="对偶四元数(Dual Quaternion)"></a>对偶四元数(Dual Quaternion)</h2><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接:"></a>参考链接:</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87418561">https://zhuanlan.zhihu.com/p/87418561</a></p>
<p><a target="_blank" rel="noopener" href="https://krasjet.github.io/quaternion/quaternion.pdf">https://krasjet.github.io/quaternion/quaternion.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://graphics.stanford.edu/courses/cs348a-17-winter/Papers/quaternion.pdf">https://graphics.stanford.edu/courses/cs348a-17-winter/Papers/quaternion.pdf</a></p>
<h2 id="code链接"><a href="#code链接" class="headerlink" title="code链接:"></a>code链接:</h2><p><a target="_blank" rel="noopener" href="https://github.com/tsinghua-rll/UR5_Controller/blob/master/RTIF/LowLevel/quaternion.py">https://github.com/tsinghua-rll/UR5_Controller/blob/master/RTIF/LowLevel/quaternion.py</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_1_overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/29/rl/rl_note_1_overview/" class="post-title-link" itemprop="url">【rl_note】1. Overview</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Difference-between-Reinforcement-Learning-and-Supervised-Learning"><a href="#Difference-between-Reinforcement-Learning-and-Supervised-Learning" class="headerlink" title="Difference between Reinforcement Learning and Supervised Learning"></a>Difference between Reinforcement Learning and Supervised Learning</h2><ul>
<li><p>Sequential data as input (not i.i.d)</p>
</li>
<li><p>The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p>
</li>
<li><p>Trial-and-error exploration (balance between exploration and exploitation)</p>
</li>
<li><p>There is no supervisor, only a reward signal, which is also delayed</p>
</li>
</ul>
<h2 id="Features-of-Reinforcement-Learning"><a href="#Features-of-Reinforcement-Learning" class="headerlink" title="Features of Reinforcement Learning"></a>Features of Reinforcement Learning</h2><ul>
<li>Trial-and-error exploration</li>
<li>Delayed reward</li>
<li>Time matters (sequential data, non i.i.d data)</li>
<li>Agent’s actions affect the subsequent data it receives (agent’s action changes the environment)</li>
</ul>
<h2 id="Introduction-to-Sequential-Decision-Making"><a href="#Introduction-to-Sequential-Decision-Making" class="headerlink" title="Introduction to Sequential Decision Making"></a>Introduction to Sequential Decision Making</h2><h3 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h3><ul>
<li>A reward is a scalar feedback signal</li>
<li>Indicate how well agent is doing at step t</li>
<li>Reinforcement Learning is based on the maximization of rewards:<br>All goals of the agent can be described by the maximization of expected<br>cumulative reward.</li>
</ul>
<h3 id="Sequential-Decision-Making"><a href="#Sequential-Decision-Making" class="headerlink" title="Sequential Decision Making"></a>Sequential Decision Making</h3><ul>
<li>Objective of the agent: select a series of actions to maximize total<br>  future rewards</li>
<li>Actions may have long term consequences</li>
<li>Reward may be delayed</li>
<li>Trade-off between immediate reward and long-term reward</li>
</ul>
<h3 id="Sequential-Decision-Making-1"><a href="#Sequential-Decision-Making-1" class="headerlink" title="Sequential Decision Making"></a>Sequential Decision Making</h3><ul>
<li><p>The history is the sequence of observations, actions, rewards.<br>$$H_t &#x3D; O_1, R_1, A_1, …, A_{t-1}, O_t, R_t$$</p>
</li>
<li><p>What happens next depends on the history</p>
</li>
<li><p>State is the function used to determine what happens next<br>$$ S_t &#x3D; f(H_t)$$</p>
</li>
</ul>
<h3 id="Sequential-Decision-Making-2"><a href="#Sequential-Decision-Making-2" class="headerlink" title="Sequential Decision Making"></a>Sequential Decision Making</h3><ul>
<li><p>Environment state and agent state:<br>$$S^e_t &#x3D; f^e(H_t)$$<br>$$S^a_t &#x3D; f^a(H_t)$$</p>
</li>
<li><p><strong>Full observability</strong> : agent directly observes the environment state<br>$$O_t &#x3D; S^e_t &#x3D; S^a_t$$</p>
</li>
<li><p><strong>Partial observability</strong> : agent indirectly observes the environment, formally as partially observable Markov decision process (POMDP)<br>$$O_t \neq S^e_t \neq S^a_t$$</p>
</li>
</ul>
<h2 id="Major-Components-of-an-RL-Agent"><a href="#Major-Components-of-an-RL-Agent" class="headerlink" title="Major Components of an RL Agent"></a>Major Components of an RL Agent</h2><ul>
<li><p><strong>Policy</strong>:</p>
<p>​    agent’s behavior function</p>
</li>
<li><p><strong>Value function</strong>: </p>
<p>​    how good is each state or action</p>
</li>
<li><p><strong>Model</strong>:</p>
<p>​    agent’s state representation of the environment</p>
</li>
</ul>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><ul>
<li><p>A policy is the agent’s behavior model</p>
</li>
<li><p>It is a map function from state&#x2F;observation to action.</p>
</li>
<li><p>Stochastic policy: Probabilistic sample<br>$$\pi(a \mid s) &#x3D; P[A_t &#x3D; a\mid S_t &#x3D; s]$$</p>
</li>
<li><p>Deterministic policy:</p>
<p>$$a^* &#x3D; \arg\max\limits_a \pi(a\mid s)$$</p>
</li>
</ul>
<h3 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h3><ul>
<li><p>Value function: expected discounted sum of future rewards under a<br>  particular policy $\pi$</p>
</li>
<li><p>Discount factor weights immediate vs future rewards</p>
</li>
<li><p>Used to quantify goodness&#x2F;badness of <strong>states</strong></p>
<p>  $$G_t &#x3D; R_{t+1}+\gamma R_{t+2}+ … &#x3D; \sum\limits_{k&#x3D;0}<br>  ^\infty\gamma^kR_{t+k+1}$$<br>  <strong>$G_t$is the total step discounted reward from time step $t$.</strong><br>  $$V_\pi(s) &#x3D; \mathbb{E}_\pi[G_t\mid S_t&#x3D;s]$$</p>
<p>  $$ &#x3D; \mathbb{E}<em>\pi[\sum\limits^\infty</em>{k&#x3D;0}\gamma^kR_{t+k+1}\mid S_t&#x3D;s]$$</p>
</li>
<li><p>Q-function at state $s$ and action $a$</p>
<p>  $$Q_\pi(s,a) &#x3D; \mathbb{E}<em>\pi[G_t\mid S_t&#x3D;s, A_t &#x3D; a] &#x3D; \mathbb{E}</em>\pi[\sum\limits^\infty_{k&#x3D;0}\gamma^kR_{t+k+1}\mid S_t&#x3D;s, A_t&#x3D;a]$$</p>
</li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>A model predicts what the environment will do next</p>
<ul>
<li><p>Predict the next state:(转移概率)<br>$$P^a_{ss’} &#x3D; P[S_{t+1}&#x3D;s’ \mid S_t&#x3D;s, A_t&#x3D;a]$$</p>
</li>
<li><p>Predict the next reward:</p>
<p>$$R^a_s &#x3D; P[R_{t+1} \mid S_t&#x3D;s, A_t&#x3D;a]$$</p>
</li>
</ul>
<h2 id="Types-of-RL-Agents-based-on-What-the-Agent-Learns"><a href="#Types-of-RL-Agents-based-on-What-the-Agent-Learns" class="headerlink" title="Types of RL Agents based on What the Agent Learns"></a>Types of RL Agents based on What the Agent Learns</h2><ul>
<li>Value-based agent:<ul>
<li>Explicit: Value function</li>
<li>Implicit: Policy (can derive a policy from value function)</li>
</ul>
</li>
<li>Policy-based agent:<ul>
<li>Explicit: policy</li>
<li>No value function</li>
</ul>
</li>
<li>Actor-Critic agent:<ul>
<li>Explicit: policy and value function</li>
</ul>
</li>
</ul>
<h2 id="Types-of-RL-Agents-on-if-there-is-model"><a href="#Types-of-RL-Agents-on-if-there-is-model" class="headerlink" title="Types of RL Agents on if there is model"></a>Types of RL Agents on if there is model</h2><ul>
<li>Model-based<ul>
<li>Explicit: model</li>
<li>May or may not have policy and&#x2F;or value function</li>
</ul>
</li>
<li>Model-free<ul>
<li>Explicit: value function and&#x2F;or policy function</li>
<li>No model.</li>
</ul>
</li>
</ul>
<h2 id="Two-Fundamental-Problems-in-Sequential-Decision-Making"><a href="#Two-Fundamental-Problems-in-Sequential-Decision-Making" class="headerlink" title="Two Fundamental Problems in Sequential Decision Making"></a>Two Fundamental Problems in Sequential Decision Making</h2><ul>
<li><p>Planning</p>
<ul>
<li>Given model about how the environment works.</li>
<li>Compute how to act to maximize expected reward without external interaction.</li>
</ul>
</li>
<li><p>Reinforcement learning</p>
<ul>
<li>Agent doesn’t know how world works</li>
<li>Interacts with world to implicitly learn how world works</li>
<li>Agent improves policy (also involves planning)</li>
</ul>
</li>
</ul>
<h2 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h2><ul>
<li>Agent only experiences what happens for the actions it tries!</li>
<li>How should an RL agent balance its actions?<ul>
<li>Exploration: trying new things that might enable the agent to make better<br> decisions in the future</li>
<li>Exploitation: choosing actions that are expected to yield good reward given<br> the past experience</li>
</ul>
</li>
<li>Often there may be an exploration-exploitation trade-off<ul>
<li>May have to sacrifice reward in order to explore &amp; learn about potentially<br> better policy</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_2_mdp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/29/rl/rl_note_2_mdp/" class="post-title-link" itemprop="url">【rl_note】2. Markov Decision Processes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><ul>
<li>Markov Processes</li>
<li>Markov Reward Processes(MRPs)</li>
<li>Markov Decision Processes (MDPs)</li>
<li>v(s)，q(s,a) transform</li>
<li>policy iteration</li>
<li>value iteration</li>
</ul>
<h2 id="Markov"><a href="#Markov" class="headerlink" title="Markov"></a>Markov</h2><h3 id="MP-S-P"><a href="#MP-S-P" class="headerlink" title="MP (S,P)"></a>MP (S,P)</h3><p>The future is independent of the past given the present</p>
<ul>
<li>history state $h_t &#x3D; {s_1,s_2,s_3…,s_t}$</li>
<li>$P(s_{t+1}\mid s_t) &#x3D; P(s_{t+1}\mid h_t)$</li>
<li>$P(s_{t+1}\mid s_t, a_t) &#x3D; P(s_{t+1}\mid h_t, a_t)$</li>
<li>State transition matrix P specifies<br>  $$p(s_{t+1} &#x3D; s’ |s_t &#x3D; s)$$</li>
</ul>
<h3 id="MRP-S-P-R-gamma"><a href="#MRP-S-P-R-gamma" class="headerlink" title="MRP (S,P,R, $\gamma$)"></a>MRP (S,P,R, $\gamma$)</h3><ul>
<li>Markov Chain + reward</li>
<li>Definition of Markov Reward Process (MRP)<ul>
<li>$S$ is a (finite) set of states ($s ∈ S$)</li>
<li>P is dynamics&#x2F;transition model that specifies<br>  $$P(S_{t+1} &#x3D; s’ |s_t &#x3D; s)$$</li>
<li>R is a reward function<br>  $$R(s_t &#x3D; s) &#x3D; \mathbb{E}[r_t \mid s_t &#x3D; s]$$</li>
<li>Discount factor $\gamma \in [0,1]$</li>
</ul>
</li>
<li>Value Function<ul>
<li><p>$$G_t &#x3D; R_{t+1}+\gamma R_{t+2}+ … &#x3D; \sum\gamma^kR_{t+k+1}$$</p>
<p>   $$V_\pi(s) &#x3D; \mathbb{E}_\pi[G_t\mid S_t&#x3D;s]$$</p>
</li>
<li><p><strong>Bellman equation</strong></p>
<p>  $$V_\pi(s) &#x3D; R_{t+1} + \gamma\sum P(s’\mid s)V(s’)$$</p>
</li>
<li><p>$$ V &#x3D; R + \gamma P V$$<br>  $$ V &#x3D; (1-\gamma P)^{-1}R$$<br>  <strong>Note</strong>Matrix inverse takes the complexity $O(N^3 )$ for N states</p>
</li>
</ul>
</li>
</ul>
<h3 id="MDP-S-P-A-R-gamma"><a href="#MDP-S-P-A-R-gamma" class="headerlink" title="MDP (S,P,A,R, $\gamma$)"></a>MDP (S,P,A,R, $\gamma$)</h3><ul>
<li>MRP + decisions $A$.</li>
<li>Definaation<ul>
<li>$S$ is a finite set of states</li>
<li>$A$ is a finite set of actions</li>
<li>$P^a$ is dynamics&#x2F;transition model for each action<br>  $$P(s_{t+1} &#x3D; s’ |s_t &#x3D; s, a_t &#x3D; a)</li>
<li></li>
<li>$R$ is a reward function<br>  $$R(s_t &#x3D; s, a_t &#x3D; a) &#x3D; \mathbb{E}[r_t \mid s_t &#x3D; s, a_t &#x3D; a]</li>
<li></li>
<li>Discount factor $\gamma \in [0, 1]$</li>
</ul>
</li>
<li>Policy in MDP<ul>
<li><p>$\pi(a|s) &#x3D; P(a_t &#x3D; a|s_t &#x3D; s)$</p>
</li>
<li><p>$P_\pi(s’|s) &#x3D; \sum\limits_{a \in A}\pi(a|s)P(s‘|s, a) 　　　a\in A$  </p>
</li>
<li><p>$R(s) &#x3D; \sum\limits_{a \in A} (a|s)R(s,a)$</p>
</li>
</ul>
</li>
<li>Value Function<ul>
<li><p>state value function</p>
<p>  用来衡量当前状态的价值<br>  $$V^\pi &#x3D; \mathbb{E}_\pi[G_t \mid s_t &#x3D; s]$$</p>
</li>
<li><p>action value function</p>
<p>  用来衡量状态s下a的价值<br>  $$q^\pi(s,a) &#x3D; \mathbb{E}_\pi[G_t \mid s_t &#x3D;s, a_t &#x3D; a]$$</p>
</li>
<li><p>$v^\pi(s) \And q^\pi(s,a)$<br>  $$ v^\pi(s) &#x3D; \sum\limits_{a\in A} \pi(a | s)q^\pi(s,a) $$</p>
</li>
<li><p>状态s 和 动作 a 之间的转换<br>  <img src="/images/posts/rl/mdp/compare_mp_mdp.png"><br>  <img src="/images/posts/rl/mdp/one.png"><br>  <img src="/images/posts/rl/mdp/two.png"></p>
</li>
</ul>
</li>
</ul>
<h2 id="v-s-q-s-a"><a href="#v-s-q-s-a" class="headerlink" title="v(s),q(s,a)"></a>v(s),q(s,a)</h2><p><strong>NOTE:</strong> MRP 对应 V(s), MDP 对应 Q(s,a)</p>
<h2 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h2><p>已知MDP过程，<strong>转移矩阵</strong> 和 <strong>奖励函数</strong> 都是已知的</p>
<h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><p>Bellman Expectation Equation<br><img src="/images/posts/rl/mdp/policy_iteration.png"></p>
<h4 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h4><p>iteration on the Bellman expectation backup<br>$$V_i(s) &#x3D; \sum\limits_{a \in A} \pi(a|s)(R(a,s) + \sum\limits_{s’ \in S}P(s’|s,a)v_{i-1}(s’)) $$</p>
<h4 id="policy-improvement"><a href="#policy-improvement" class="headerlink" title="policy improvement"></a>policy improvement</h4><p>greedy on action-value function q</p>
<p>$$q_{\pi_i}(s,a) &#x3D; R(s,a)+\gamma\sum\limits_{s\in S}P(s’|s,a) V_{\pi_i}(s’)$$</p>
<p>$$\pi_{i+1}(s) &#x3D;  arg \max\limits_a q_{\pi_i}(s,a)$$</p>
<h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p>Bellman Optimality Equation</p>
<p>整个过程没有策略的引入</p>
<h4 id="iteration-on-the-Bellman-optimality-backup"><a href="#iteration-on-the-Bellman-optimality-backup" class="headerlink" title="iteration on the Bellman optimality backup"></a>iteration on the Bellman optimality backup</h4><p>$$v_{i+1}(s) \larr \max\limits_{a\in A}R(s,a) + \gamma\sum\limits_{s’\in S}P(s’|s,a)v_i(s’) $$</p>
<h4 id="retrieve-the-optimal-policy-after-the-value-iteration"><a href="#retrieve-the-optimal-policy-after-the-value-iteration" class="headerlink" title="retrieve the optimal policy after the value iteration"></a>retrieve the optimal policy after the value iteration</h4><p>在前面迭代收敛后再，根据 $v_{end}(s)$ 选取策略</p>
<p>$$\pi^*(s) \larr \argmax\limits_a R(s,a) + \gamma\sum\limits_{s’\in S}P(s’|s,a)v_{end}(s’) $$</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_3_model_free/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/29/rl/rl_note_3_model_free/" class="post-title-link" itemprop="url">【rl_note】3. model-free Prediction And Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="prediction"><a href="#prediction" class="headerlink" title="prediction"></a>prediction</h2><p>Evaluate the state value without knowing the MDP model, by only interacting with the environment</p>
<h3 id="MC"><a href="#MC" class="headerlink" title="MC"></a>MC</h3><h4 id="traditional"><a href="#traditional" class="headerlink" title="traditional"></a>traditional</h4><ul>
<li><p>value state of v(s) $t$ is time  that state $s_i$ is visited in an episode</p>
<p>  <strong>Note</strong> 在每个episode里面分为first-visit 和every-visit，代表每一条轨迹可能会重复出现状态$s$，是每一条轨迹只计算第一次出现，或者每次出现都会计算。</p>
</li>
<li><p>$N_0 &#x3D; 0;S_0(s_i) &#x3D; 0; G_0 &#x3D; 0$</p>
</li>
<li><p>Increment counter  $N_t(s_i) ← N_{t-1}(s_i) + 1$</p>
</li>
<li><p>Increment total return $S_t(s_i) ← S_{t-1}(s_i) + G_t$</p>
</li>
<li><p>Value is estimated by mean return $v_t(s_i) &#x3D; S_t(s_i)&#x2F;N_t(s_i)$</p>
</li>
<li><p>By law of large numbers(大数定律), 　$v(s_i) → v_\pi(s_i)$ as　$N(s_i) → ∞$</p>
</li>
</ul>
<h4 id="incremental"><a href="#incremental" class="headerlink" title="incremental"></a>incremental</h4><ul>
<li>Collect one episode $(S_1 , A_1 , R_1 , …, S_t )$</li>
<li>For each state $s_i$ with computed return $G_t$</li>
<li>$N_0(s_i) &#x3D; 0; G_0 &#x3D; 0; v_0(s_i) &#x3D; 0$</li>
<li>$N_t(s_i) \leftarrow N_{t-1}(s_i) + 1$</li>
<li>$v_t(s_i) \leftarrow v_{t-1}(s_i) + 1&#x2F;N_t(G_t -v_{t-1}(s_i)))$<ul>
<li>$v_t(s_i) \leftarrow v_{t-1}(s_i) + \alpha(G_t -v_{t-1}(s_i)))$</li>
<li>$\alpha$ 也被叫做学习率， 区别于监督学习的 <strong>learning rate</strong></li>
</ul>
</li>
</ul>
<h2 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a>TD(0)</h2><p><strong>Objective:</strong> learn $v_\pi$ online from experience under policy $\pi$</p>
<ul>
<li><p>Update $v_t(s)$ toward estimated return $R + \gamma v_{t-1}(s’)$</p>
<p>  <strong>Note</strong> 这儿是 $R$ 是 $s\rightarrow s’$产生的奖励</p>
</li>
<li><p>$v_t(s) \leftarrow v_{t-1}(s) + \alpha (R + \gamma v_{t-1}(s’) − v_{t-1}(s))$</p>
</li>
<li><p><strong>TD target:</strong> $R + \gamma v_{t-1}(s’)$</p>
</li>
<li><p><strong>TD error:</strong> $\delta_t &#x3D; R + \gamma v_{t-1}(s’) − v_{t-1}(s)$</p>
<ul>
<li>$v_\pi\doteq \mathbb{E}_\pi[G_t|S_t &#x3D; s]$</li>
<li>$&#x3D;\mathbb{E}<em>\pi[R</em>{t+1} + \gamma G_{t+1}|S_t &#x3D; s]$</li>
<li>$&#x3D;\mathbb{E}<em>\pi[R</em>{t+1} + \gamma v_\pi(S_{t+1})|S_t &#x3D; s]$</li>
<li>MC将式一估计值作为目标, 之所以叫估计值， 是因为$G_t$ 的期望是未知的，需要在采样得到样本期望值</li>
<li>DP则是对式三值作为目标，之所以叫估计值， 不是因为期望的原因，会假定环境会完整的提供期望值， 这儿 $v_\pi(S_{t+1})$ 是未知的 需要用当前的 $V(S_{t+1})$</li>
<li>TD 也是估计值, 通过采样得到式三的期望 , 使用当前的估计值 $V$ 来代替真实值 $v_\pi$</li>
</ul>
</li>
</ul>
<p><strong>NOTE:</strong> TD error 是一种误差， 衡量 $s$ 的估计值与更好的估计值 $R + \gamma v_{t-1}(s’)$之间的差距。 因为用到了$v_{t-1}(s’)$了， 就涉及到 <strong>bootstrapping</strong> 的思想，即基于前一时刻的除本身要更新的 $v_{t-1}(s)$ 外的其他值，有点动态规划的味道</p>
<h3 id="compare-DP-MC-TD"><a href="#compare-DP-MC-TD" class="headerlink" title="compare DP MC TD"></a>compare DP MC TD</h3><p>结合上面TD error 的 Note</p>
<p><img src="/images/posts/rl/model_free/compare_dp_mc_td.png"></p>
<p><img src="/images/posts/rl/model_free/compare_dp_mc_td_1.png"></p>
<ul>
<li>MC does not bootstrap, DP TD  bootstraps</li>
<li>MC TD samples, DP does not sample</li>
<li>TD advantages over MC<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Incomplete sequences</li>
</ul>
</li>
</ul>
<h2 id="control"><a href="#control" class="headerlink" title="control"></a>control</h2><h3 id="On-policy-Off-policy"><a href="#On-policy-Off-policy" class="headerlink" title="On-policy Off-policy"></a>On-policy Off-policy</h3><ul>
<li><p>On-policy learning</p>
<p>“Learn on the job”</p>
<p>Learn about policy π from experience sampled from π</p>
</li>
<li><p>Off-policy learning</p>
<p>“Look over someone’s shoulder”</p>
<p>Learn about policy π from experience sampled from μ</p>
</li>
</ul>
<h3 id="sarsa"><a href="#sarsa" class="headerlink" title="sarsa"></a>sarsa</h3><p><img src="/images/posts/rl/model_free/sarsa_control.png"></p>
<p><img src="/images/posts/rl/model_free/sarsa_al.png"></p>
<h3 id="q-learning"><a href="#q-learning" class="headerlink" title="q-learning"></a>q-learning</h3><p><img src="/images/posts/rl/model_free/q_learning_al.png"></p>
<h3 id="compare-sarsa-q-learning"><a href="#compare-sarsa-q-learning" class="headerlink" title="compare sarsa q-learning"></a>compare sarsa q-learning</h3><p><img src="/images/posts/rl/model_free/diagram_sarsa_qlearning.png"></p>
<p><strong>Sarsa:</strong> </p>
<ul>
<li>On-Policy TD control</li>
<li>Choose action $A_t$ from $S_t$ using policy derived from Q with $\epsilon$-greedy</li>
<li>Take action $A_t$ , observe $R_{t+1}$ and $S_{t+1}$</li>
<li>Choose action $A_{t+1}$ from $S_{t+1}$ using policy derived from Q with $\epsilon$-greedy</li>
<li>$Q(S_t,A_t) \leftarrow Q(S_t , A_t ) + \alpha[R_{t+1} + \gamma Q(S_{t+1} , A_{t+1}) − Q(S_t , A_t)]$</li>
<li><strong>note:</strong> $A$ and $A’$ are sampled from the same policy so it is on-policy</li>
</ul>
<p><strong>Q-Learning:</strong></p>
<ul>
<li>Off-Policy TD control</li>
<li>Choose action $A_t$ from $S_t$ using policy derived from Q with $\epsilon$-greedy</li>
<li>Take action $A_t$ , observe $R_{t+1}$ and $S_{t+1}$</li>
<li>Then ‘imagine’ $A_{t+1}$ as $argmax Q(S_{t+1} , a’ )$ in the update target </li>
<li>$Q(S_t , A_t ) \leftarrow Q(S_t , A_t ) + \alpha[R_{t+1} + \gamma \max\limits_a Q(S_{t+1} , a) − Q(S_t , A_t)]$</li>
<li><strong>note:</strong> In Q Learning, $A$ and $A’$ are from different policies, with $A$ being<br>more exploratory and $A’$ determined directly by the max operator</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_4_value_function_approximation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/29/rl/rl_note_4_value_function_approximation/" class="post-title-link" itemprop="url">【rl_note】4. Value Function Approximation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Challenges-with-large-MDPs"><a href="#Challenges-with-large-MDPs" class="headerlink" title="Challenges with large MDPs:"></a>Challenges with large MDPs:</h2><ul>
<li>too many states or actions to store in memory</li>
<li>too slow to learn the value of each state individually</li>
</ul>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><ul>
<li><p>为了避免通过查表的方式表示Value function, state-action function，Policy</p>
</li>
<li><p>Estimate with function approximation</p>
<ul>
<li>$\hat{v} (s, \mathbf{\omega}) \approx  v^π (s)$</li>
<li>$\hat{q}(s, a, \mathbf{\omega}) \approx  q^π (s, a)$</li>
<li>$\hat{\pi}(a, s, \mathbf{\omega}) \approx  π(a \mid s)$</li>
</ul>
</li>
<li><p>Linear VFA often works well given the right set of features, But it requires manual designing of the feature set</p>
</li>
<li><p>Alternative is to use a much richer function approximator that is able to directly learn from states without requiring the feature design</p>
</li>
<li><p>Nonlinear function approximator: Deep neural networks</p>
</li>
</ul>
<h2 id="Value-Function-Approximation-with-an-Oracle"><a href="#Value-Function-Approximation-with-an-Oracle" class="headerlink" title="Value Function Approximation with an Oracle"></a>Value Function Approximation with an Oracle</h2><ul>
<li>We assume that we have the oracle for knowing the true value for<br>$v^π (s)$ for any given state $s$</li>
<li>Then the objective is to find the best approximate representation of $v^π (s)$</li>
<li>Thus use the <strong>mean squared error</strong> and define the loss function as<br>$$J(\omega) &#x3D; \mathbb{E}_π [(v^π (s) − \hat{v}(s, w))^2]$$</li>
<li>Follow the gradient descend to find a local minimum<br>$$\Delta \omega &#x3D; − {1 \over 2} \alpha ∇<em>\omega J(\omega)$$<br>$$\omega</em>{t+1} &#x3D; \omega_t + \Delta \omega$$</li>
</ul>
<h2 id="Linear-Value-Function-Approximation"><a href="#Linear-Value-Function-Approximation" class="headerlink" title="Linear Value Function Approximation"></a>Linear Value Function Approximation</h2><ul>
<li>Represent value function by a linear combination of features, s is feature vector<br>$$\hat{v}(s,\omega) &#x3D; x(s)^T \omega &#x3D; \sum_{j&#x3D;1}^n x_j(s)\omega_j$$</li>
<li>The objective function is quadratic in parameter $\omega$<br>$$J(\omega) &#x3D; \mathbb{E}_\pi[v^\pi(s) − x(s)^T \omega]^2$$</li>
<li>Thus the update rule is as simple as<br>$$\Delta\omega &#x3D; \alpha(\mathbf{v^\pi(s)} − \hat{v}(s, \omega))x(s)$$<br>$$Update &#x3D; StepSize × PredictionError × FeatureValue$$</li>
<li>Stochastic gradient descent converges to global optimum. Because in the linear case, there is only one optimum, thus local optimum is automatically converge to or near the global optimum.</li>
<li>Special case<ul>
<li><p>Table lookup is a special case of linear value function approximation</p>
</li>
<li><p>Table lookup feature is one-hot vector <strong>(只有一个元素为1)</strong> as follows</p>
<p>  $$x^{table}(s) &#x3D; [0(s &#x3D; s_1 ), …, 1(s &#x3D; s_n )]^T$$</p>
</li>
<li><p>Then we can see that each element on the parameter vector $w$ indicates the value of each individual state</p>
<p>  $$\hat{v}(s, \omega) &#x3D; [0(s &#x3D; s_1 ), …, 1(s &#x3D; s_n)] [w_1 , …, w_n]^T$$</p>
</li>
<li><p>Thus we have（对于的权重就是对应k的权重）<br>  $$\hat{v}(s_k , \omega) &#x3D; \omega_k$$</p>
</li>
</ul>
</li>
</ul>
<h2 id="No-Oracle"><a href="#No-Oracle" class="headerlink" title="No Oracle"></a>No Oracle</h2><ul>
<li><p>For MC, the target is the actual return $G_t$</p>
<p>  $$\Delta\omega &#x3D; \alpha (\mathbf{G_t} − \hat{v}(s_t , \omega) ∇\omega \hat{v}(s_t , \omega)$$</p>
<p>  <strong>$G_t$</strong> 因为是采样，所以这儿是无偏的估计， 但是会有噪声</p>
</li>
<li><p>For TD(0), the target is the TD target $R_{t+1} + \gamma \hat{v}(s_{t+1} , \mathbf{\omega})$<br>  $$\Delta\omega &#x3D; \alpha(\mathbf{R_{t+1} +\gamma \hat{v} (s_{t+1} , \omega)} − \hat{v}(s_t , \omega) ∇ \omega\hat{v} (s_t , \omega)$$<br>  <strong>TD target</strong> 有偏估计， 因为用到了旧时刻的 $\omega$</p>
</li>
</ul>
<p>Generalized policy iteration<br><img src="/images/posts/rl/value_function_approximation/policy_iteration.png"></p>
<h3 id="同理-v-s-可替换为-q-s-a"><a href="#同理-v-s-可替换为-q-s-a" class="headerlink" title="同理 $v(s)$ 可替换为 $q(s,a)$"></a>同理 $v(s)$ 可替换为 $q(s,a)$</h3><ul>
<li><p>MC<br>  $$\Delta\omega &#x3D; \alpha (\mathbf{G_t} − \hat{q}(s_t ,a_t, \omega) ∇\omega \hat{q}(s_t ,a_t, \omega)$$</p>
</li>
<li><p>sarsa<br>  $$\Delta\omega &#x3D; \alpha(\mathbf{R_{t+1} +\gamma \hat{q} (s_{t+1} ,a_{t+1}, \omega)} − \hat{q}(s_t,a_t,\omega) ∇ \omega\hat{q} (s_t, a_t, \omega)$$</p>
<p>  <strong>Note:</strong> 训练困难</p>
<ul>
<li>在优化过程中，即近似 <strong>Bellman backup</strong> 又近似 <strong>value function</strong>, </li>
<li>off-policy control: <strong>behavior policy</strong> and <strong>target policy</strong></li>
</ul>
</li>
<li><p>q-learning<br>  $$\Delta\omega &#x3D; \alpha(\mathbf{R_{t+1} +\gamma \max_a \hat{q}(s_{t+1} ,a, \omega)} − \hat{q}(s_t,a_t,\omega) ∇ \omega\hat{q} (s_t, a_t, \omega)$$</p>
</li>
</ul>
<p><img src="/images/posts/rl/value_function_approximation/sarsa_semi_gradient.png"></p>
<h3 id="Deadly-Triad"><a href="#Deadly-Triad" class="headerlink" title="Deadly Triad"></a>Deadly Triad</h3><ul>
<li>Function approximation: 利用函数估计会有误差</li>
<li>Bootstrapping: 主要影响 TD 和 MP</li>
<li>Off-policy training:  <strong>behavior policy</strong> and <strong>target policy</strong></li>
</ul>
<h2 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a>Convergence</h2><p><img src="/images/posts/rl/value_function_approximation/convergence.png"></p>
<hr>
<p><img src="/images/posts/rl/value_function_approximation/convergence_al.png"></p>
<h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/29/rl/rl_note_5_policy_gradent/" class="post-title-link" itemprop="url">【rl_note】5. Policy Gradent(AC, TRPO, PPO)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Policy-Gradent"><a href="#Policy-Gradent" class="headerlink" title="Policy Gradent"></a>Policy Gradent</h1><h2 id="Value-based-RL-vs-Policy-based-RL"><a href="#Value-based-RL-vs-Policy-based-RL" class="headerlink" title="Value-based RL vs Policy-based RL"></a>Value-based RL vs Policy-based RL</h2><ul>
<li>value base 主要是学习一个value function 的近似，然后通过贪心的方法得到最优策略<br>  $$a_t&#x3D;\arg \max_aQ(a,st)$$</li>
<li>policy base 直接学习策略$\pi_\theta(a|s)$, 学习参数$\theta$<ul>
<li>Advantages:<ul>
<li>better convergence properties: we are guaranteed to converge on a local optimum (worst case) or global optimum (best case)</li>
<li>Policy gradient is more effective in high-dimensional action space</li>
<li>Policy gradient can learn stochastic policies, while value function can’t</li>
</ul>
</li>
<li>Disadvantages:<ul>
<li>局部最忧</li>
<li>方差大</li>
</ul>
</li>
<li>Deterministic: given a state, the policy returns a certain action to take</li>
<li>Stochastic:<ul>
<li>probability distribution of distribute actions</li>
<li>a certain Gaussian distribution for continuous action</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Policy-Gradent-for-Multi-step-MDPs"><a href="#Policy-Gradent-for-Multi-step-MDPs" class="headerlink" title="Policy Gradent for Multi-step MDPs"></a>Policy Gradent for Multi-step MDPs</h2><ul>
<li><p>轨迹序列:<br>  $$\tau &#x3D; (s_0, a_0, r_0, … ,s_{T-1}, a_{T-1},r_{T-1},s_T, r_T) \sim (\pi_\theta, P(s_{t+1} | s_t, a_t))$$</p>
</li>
<li><p>轨迹概率:</p>
<p>  $$P(\tau;\theta) &#x3D; \mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$$</p>
<p>  $\pi_\theta(a_t|s_t)$ 仅该项和 Policy 有关</p>
<p>  $p(s_{t+1}|s_t, a_t)$ 其为动力学模型， 仅与环境有关</p>
</li>
<li><p>累计回报:<br>  $$R(\tau) &#x3D; \sum\limits_{t&#x3D;0}^T r_t$$</p>
</li>
<li><p>优化目标:<br>  $$J(\theta) &#x3D; \sum\limits_\tau R(\tau) P(\tau;\theta)$$<br>  对轨迹$\tau$进行累加</p>
</li>
<li><p>Goal</p>
<ul>
<li><p>goal:<br>$$\theta^* &#x3D; \arg\max\limits_\theta J(\theta) &#x3D; \arg\max\limits_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
</li>
<li><p>gradient:</p>
<p>$$\nabla_\theta Ja(\theta) &#x3D;\nabla_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
<p>$$&#x3D;\sum\limits_\tau R(\tau)  P(\tau;\theta) \nabla_\theta logP(\tau;\theta)$$</p>
</li>
<li><p>Monte carlo:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m R(\tau_i) \nabla_\theta logP(\tau_i ;\theta)$$</p>
<p>采样后轨迹概率 $P(\tau;\theta)$ 变为采样的加和</p>
</li>
<li><p>分解 $\nabla_\theta logP(\tau_i ;\theta)$:</p>
<p>$$\nabla_\theta logP(\tau_i ;\theta) &#x3D; \nabla_\theta log[\mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)]$$<br>$$&#x3D;\nabla_\theta[log(\mu(s_0)) + \sum\limits_{t&#x3D;0}^{T-1} (\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)))]$$<br>$$&#x3D;\sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$$</p>
<p>$\theta$仅与policy有关  </p>
</li>
<li><p>Finaly:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m [R(\tau_i) \sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)]$$</p>
<p>$\nabla_\theta \log\pi_\theta(a_t^i|s_t^i)$作为<strong>score function</strong>代表梯度更新方向</p>
<p>$R(\tau_i)$表示更新方向上的权重， 偏向于好的轨迹</p>
</li>
</ul>
</li>
</ul>
<h2 id="Comparison-to-Maximum-Likelihood"><a href="#Comparison-to-Maximum-Likelihood" class="headerlink" title="Comparison to Maximum Likelihood"></a>Comparison to Maximum Likelihood</h2><ul>
<li><p>PG<br>  $$\nabla_\theta J(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m)) (\sum\limits_{t&#x3D;1}^{T}r(s_t^m, a_t^m))$$</p>
</li>
<li><p>Maximum Likelihood<br>  $$\nabla_\theta J_{ML}(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m))$$</p>
</li>
<li><p>往更好的轨迹偏移</p>
<p>  <img src="/images/posts/rl/policy_gradent/pg_ml.png"></p>
</li>
</ul>
<h2 id="Reduce-Variance"><a href="#Reduce-Variance" class="headerlink" title="Reduce Variance"></a>Reduce Variance</h2><ul>
<li><h2 id="temporal-causality-利用当前时刻后的奖励而不用整条轨迹的奖励-G-t-i-sum-limits-t’-t-T-1-r-t’-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i"><a href="#temporal-causality-利用当前时刻后的奖励而不用整条轨迹的奖励-G-t-i-sum-limits-t’-t-T-1-r-t’-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i" class="headerlink" title="temporal causality  - 利用当前时刻后的奖励而不用整条轨迹的奖励 $G_t^i &#x3D; \sum\limits_{t’&#x3D;t}^{T-1}r_{t’}^i$  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$"></a>temporal causality<br>  - 利用当前时刻后的奖励而不用整条轨迹的奖励 $G_t^i &#x3D; \sum\limits_{t’&#x3D;t}^{T-1}r_{t’}^i$<br>  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</h2><ul>
<li>REINFORCE:<br>  <img src="/images/posts/rl/policy_gradent/reinforence.png"></li>
<li>n-step<br>  $$G_t^{(1)} &#x3D; r_{t+1} + \gamma v(s_{t+1})$$<br>  $$G_t^{(2)} &#x3D; r_{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})$$<br>  $$G_t^{(\infty)} &#x3D; r_{t+1} + \gamma r_{t+2} + … +\gamma^{T-t-1}r_T$$</li>
</ul>
</li>
<li>add baseline<ul>
<li>利用当前时刻后的奖励 - 平均奖励，而不用整条轨迹的奖励$G_t^i -b^i(s_t)$</li>
<li>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} (G_t^i-b^i(s_t)) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</li>
</ul>
</li>
</ul>
<h2 id="PG-Algorithm"><a href="#PG-Algorithm" class="headerlink" title="PG Algorithm"></a>PG Algorithm</h2><p><img src="/images/posts/rl/policy_gradent/pg_al.png"></p>
<h1 id="Problem-of-PG"><a href="#Problem-of-PG" class="headerlink" title="Problem of PG"></a>Problem of PG</h1><ul>
<li>训练不稳定<ul>
<li>Trust region (TRPO, PPO)</li>
<li>natural policy gradient (二阶优化方法)</li>
</ul>
</li>
<li>如何离线化训练 <strong>off-policy</strong><ul>
<li>Importance sampling（将分布转换到已知的分布）</li>
</ul>
</li>
</ul>
<h1 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h1><ul>
<li><p>利用当前状态的<strong>Advantage function</strong>作为权重 ，而不用整条轨迹的奖励<br>  一般通过参数 $w$ 来拟合<br>  $$A(s_t^i, a_t^i) &#x3D; Q(s_t^i, a_t^i) - V(s_t^i)$$</p>
</li>
<li><p>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} A_w(s_t^i,a_t^i) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</p>
</li>
<li><p>n step<br>$$\hat{A}<em>t^{(1)} &#x3D; r</em>{t+1} + \gamma v(s_{t+1})- v(s_t)$$<br>$$\hat{A}<em>t^{(2)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})- v(s_t)$$<br>$$\hat{A}<em>t^{(\infty)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + … +\gamma^{T-t-1}r_T- v(s_t)$$<br>$\hat{A}_t^{(1)}$ low variance ,high bias</p>
<p>$\hat{A}_t^{(\infty)}$ high variance ,low bias</p>
</li>
</ul>
<h1 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h1><p>PG + off-policy + constrain</p>
<ul>
<li><p>步长的重要性</p>
<ul>
<li>状态和回报在改变统计特性, 优化过程中很难确定更新步长</li>
<li>policy 经常会过早陷入次忧的几乎确定的策略中</li>
<li>TRPO 通过置信域的方式保证步长为单调不减</li>
<li><img src="/images/posts/rl/policy_gradent/trust_region.png"></li>
</ul>
</li>
<li><p>目标函数(带有折扣的奖励最大)：</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) &#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^{\infty} \gamma r(s_t)] $$</p>
</li>
<li><p>新策略下的期望回报 &#x3D; 旧策略下的期望回报 + 新旧策略期望回报的差<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [Q_\pi(s_t,a_t)-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [r(s_t)+ \gamma V_\pi(s_{t+1})-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)+ \sum\limits_{t&#x3D;0}^\infty (\gamma V_\pi(s_{t+1})-V_\pi(s_t))]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)] + \mathbb{E}<em>{s_0}[-V</em>\pi(s_0)]$求和展开前后相消</p>
<p>  $&#x3D; \eta(\tilde{\pi}) - \eta(\pi)$</p>
</li>
</ul>
</li>
<li><p>对于上式展开 </p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $$\rho_\pi(s) &#x3D; P(s_0&#x3D;s) + \gamma P(s_1&#x3D;s) + \gamma^2P(s_2&#x3D;s) + … &#x3D; \sum\limits_{t&#x3D;0}^\infty \gamma^t P(s_t&#x3D;s|\tilde{\pi} )$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum_{t&#x3D;0}^\infty \sum\limits_{s} P(s_t &#x3D; s|\tilde{\pi})\sum\limits_a \tilde{\pi}(a|s) \gamma^t A_\pi(s,a)$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$<br>  这是状态分布由新的策略$\tilde{\pi}$， 严重依赖新的状态分布</p>
</li>
<li><p>two trick</p>
<ul>
<li><p>忽略状态分布的变化， 假定<br>  $$\rho_{\tilde{\pi}}(s)\sim\rho_\pi(s)$$</p>
</li>
<li><p>重要性采样，</p>
<ul>
<li><img src="/images/posts/rl/policy_gradent/importance.png"></li>
<li>$$\sum\limits_a \tilde{\pi_\theta}(a|s) A_{\theta_{old}}(s,a) &#x3D; \mathbb{E}<em>{a\sim\pi</em>{old}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</li>
</ul>
</li>
<li><p>原目标：<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$</p>
</li>
<li><p>新目标：<br>  $$L_{\pi_{\theta_{old}}} (\tilde{\pi})&#x3D; \eta(\pi) + \mathbb{E}<em>{s\sim \rho</em>{\theta {old}}, a\sim\pi_{\theta {old}}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</p>
</li>
</ul>
</li>
<li><p>梯度更新方向， 梯度优化本身就是一阶优化，</p>
<ul>
<li>在$\theta_{old}$ 处, 一阶凸优化方法中，完全一致，一阶导数一致，<br>  $$\nabla_\theta \eta (\pi_{\theta_{old}}) &#x3D; \nabla_\theta L_{\pi_{\theta_{old}}} (\pi_{\theta_{old}})$$ </li>
<li><img src="/images/posts/rl/policy_gradent/one.png"></li>
</ul>
</li>
<li><p>梯度更新步长</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) \geqslant L_\pi(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})$$</p>
<p>  $where , C &#x3D; \frac{2\epsilon\gamma}{(1-\gamma)^2}$</p>
</li>
<li><p>令 $M_{i}(\pi)  &#x3D; L_{\pi_{i}}(\pi) - CD_{KL}^{MAX} (\pi_i, \pi)$</p>
<p>  $\eta (\pi_{i+1}) \geqslant M_i(\pi_{i+1})$</p>
<p>  等效于上面不等式</p>
<p>  $\eta (\pi_{i}) &#x3D; M_i(\pi_{i})$</p>
<p>  将$\pi_i$带入不等式，两分布相同，$D_{KL}$为0</p>
<p>  $\eta (\pi_{i+1}) - \eta (\pi_{i}) \geqslant M_i(\pi_{i+1}) -M_i(\pi_{i})$</p>
<p>  即最大化$M_i$, 可保证更新步长单调不减</p>
</li>
</ul>
</li>
<li><p>Finally:</p>
<ul>
<li><p>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})]$</p>
</li>
<li><p>$\max_\theta {mize} L_{\theta_{old}}(\theta)$</p>
<p>  subject to : $D_{KL}^{MAX} (\theta_{old}, \theta) \leqslant \delta$</p>
<p>  subject to : $\bar{D}<em>{KL}^{\rho</em>{\theta_{old}}} (\theta_{old}, \theta) \leqslant \delta$</p>
</li>
<li><p>线性化逼近， 二次逼近后:</p>
<p>  $\max_\theta {mize}[\nabla_\theta L_{\theta_{old}}(\theta)|<em>{\theta &#x3D; \theta</em>{old}} \cdot (\theta-\theta_{old})]$gradent&#x2F;ppo_clip.png)</p>
<p>  subject to : $\frac{1}{2} \left | \theta- \theta_{old} \right |^2 \leqslant \delta$</p>
</li>
</ul>
</li>
</ul>
<h1 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h1><p>想比于TRPO的变化就是</p>
<ul>
<li>TRPO 用KL作为一个约束， 而 PPO 则一快和$\theta$进行优化</li>
<li><strong>Note:</strong> KL 为输出$a$的距离，而不是参数$\theta$ 的距离</li>
<li>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) -\beta KL(\pi, \tilde{\pi})]$<ul>
<li>If $𝐾𝐿(\theta, \theta_i) &gt; KL_{max}$,  increase $\beta$</li>
<li>If $𝐾𝐿(\theta, \theta_i) &lt; KL_{min}$,  decrease $\beta$</li>
</ul>
</li>
<li>优化目标：<ul>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a) - \beta KL(\pi_{old}, \pi)]$$</p>
</li>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[min(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a), clip(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon, 1+\epsilon)A_{\theta_{old}}(s,a))]$$</p>
<p>  <img src="/images/posts/rl/policy_gradent/ppo_clip.png"></p>
</li>
</ul>
</li>
</ul>
<h1 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h1><ul>
<li>目的： 让DQN应用于连续的动作空间</li>
<li></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/04/24/dl/net_param_calculate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/04/24/dl/net_param_calculate/" class="post-title-link" itemprop="url">【dl】 神经网络中参数量和计算量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-04-24 12:51:55" itemprop="dateCreated datePublished" datetime="2020-04-24T12:51:55+08:00">2020-04-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h1><p><img src="/images/posts/dl/leNet_param.png" alt="LeNet"></p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>假设输入神经元数为M，输出神经元数为N，则</p>
<ul>
<li><p>bias为True时：</p>
<p>  则参数数量为：M*N + N（bias的数量与输出神经元数的数量是一样的）</p>
</li>
<li><p>bias为False时：</p>
<p>  则参数数量为：M×N</p>
</li>
</ul>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>假设卷积核的大小为 k*k, 输入channel为M， 输出channel为N。</p>
<ul>
<li><p>bias为True时：</p>
<p>  则参数数量为：k×k×M×N + N（bias的数量与输出channel的数量是一样的）</p>
</li>
<li><p>bias为False时：</p>
<p>  则参数数量为：k×k×M×N</p>
</li>
<li><p>当使用BN时，还有两个可学习的参数α和β，参数量均为N</p>
<p>  则参数数量为：k×k×M×N + 3×N</p>
</li>
</ul>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>首先要说一下LSTM的结构</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47907312">参考链接</a></p>
<ul>
<li><p>遗忘门<br>  <img src="/images/posts/dl/forget.webp"></p>
</li>
<li><p>输入们<br>  <img src="/images/posts/dl/input.gif"></p>
</li>
<li><p>cell 状态<br>  <img src="/images/posts/dl/cell.webp"></p>
</li>
<li><p>输出门<br>  <img src="/images/posts/dl/output.webp"></p>
<p>  1，把先前隐藏状态prev_ht和当前输入input合并成combine</p>
<p>  2，把combine输入遗忘层，决定哪些不相关数据需要被剔除</p>
<p>  3，用combine创建候选层，其中包含能被添加进cell状态的可能值</p>
<p>  4，把combine输入输入层，决定把候选层中哪些信息添加进cell状态</p>
<p>  5，更新当前cell状态</p>
<p>  6，把combine输入输出层，计算输出</p>
<p>  7，把输出和当前cell状态进行点乘，得到更新后的隐藏状态</p>
</li>
</ul>
<p>因为input进入了四个不同的mlp，每一个可参考MLP的参数进行计算</p>
<p><strong>实际的LSTM模型</strong><img src="/images/posts/dl/lstm.jpg"></p>
<h1 id="计算量"><a href="#计算量" class="headerlink" title="计算量"></a>计算量</h1><h2 id="MLP-1"><a href="#MLP-1" class="headerlink" title="MLP"></a>MLP</h2><p>假设 输入神经元数为M，输出神经元数为N，则</p>
<ul>
<li><p>先执行M次乘法；</p>
</li>
<li><p>再执行M-1次加法</p>
</li>
<li><p>加上bias，计算出一个神经元的计算量为 （M+M-1+1）</p>
</li>
<li><p>N个输出神经元，则总的计算量为 2M×N</p>
</li>
</ul>
<h2 id="CNN-1"><a href="#CNN-1" class="headerlink" title="CNN"></a>CNN</h2><p>假设输入特征图（B，C，H，W），卷积核大小为K×K， 输入通道为C，输出通道为N，步长stride为S， 输出特征图大小为H2，W2.</p>
<ul>
<li><p>一次卷积的计算量</p>
<p>  一个k×k的卷积，执行一次卷积操作，需要k×k次乘法操作（卷积核中每个参数都要和特征图上的元素相乘一次），k×k−1 次加法操作（将卷积结果，k×k 个数加起来）。所以，一次卷积操作需要的乘加次数：(K×K)+(K×K−1)&#x3D;2×K×K−1</p>
</li>
<li><p>在一个特征图上需要执行卷积需要卷积的次数</p>
<p>  在一个特征图上需要执行的卷积次数：(（H-k+Ph）&#x2F;S +1 )×(（H-k+Pw）&#x2F;S +1)，Ph，Pw表示在高和宽方向填充的像素，此处假定了宽高方向滑动步长和核的宽高是一样，若不同，调整一下值即可。若不能整除，可向下取整。</p>
<p>  <strong>Note</strong>:卷积输出层的维度</p>
</li>
<li><p>C个特征图上进行卷积运算的次数</p>
<p>  C个输入特征图上进行卷积运算的次数为C</p>
</li>
<li><p>输出一个特征图通道需要的加法次数</p>
<p>  在C个输入特征图上进行卷积之后需要将卷积的结果相加，得到一个输出特征图上卷积结果，C个相加需要C-1次加法，计算量为 ：（C-1）×H2×W2</p>
</li>
<li><p>输出N个特征图需要计算的次数</p>
<p>  N×（（C-1）×H2×W2 + （2×K×K−1）×(（H-k+Ph）&#x2F;S +1 )×(（H-k+Pw）&#x2F;S +1) ×C）</p>
</li>
<li><p>一个batch需要计算的次数</p>
<p>  B×N×（（C-1）×H2×W2 + （2×K×K−1）×(（H-k+Ph）&#x2F;S +1 )×(（H-k+Pw）&#x2F;S +1) ×C）</p>
</li>
</ul>
<h3 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h3><p>具体可参考上面LSTM流程，按照MLP的计算方式去计算</p>
<p>Bingo!!!<br>一直想写，终于把这个坑填上了</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/03/24/dl/GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/03/24/dl/GAN/" class="post-title-link" itemprop="url">【dl】 GAN_公式推导</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-03-24 12:51:55" itemprop="dateCreated datePublished" datetime="2020-03-24T12:51:55+08:00">2020-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><h2 id="GAN数学描述"><a href="#GAN数学描述" class="headerlink" title="GAN数学描述"></a>GAN数学描述</h2><p><strong>目标:</strong> 以假乱真的生成器</p>
<ul>
<li>高水平的鉴赏家</li>
<li>高水平的工艺大师</li>
</ul>
<p>原始数据概率分布: </p>
<p>$$P_{data} &#x3D;(x;\theta)$$</p>
<p>生成器概率分布:<br>$$P_z &#x3D;(z;\theta_g)$$</p>
<p>这儿 $\theta_g$ 表示概率的参数，</p>
<p>生成器网络表示：</p>
<p>$$x&#x3D;G (z;\theta_g)$$</p>
<p>这儿 $\theta_g$ 表示网络的参数，这儿不对生成器进行具体建模，用一个神经网络(<strong>确定性函数</strong>)去描述 </p>
<p>鉴别器网络表示：<strong>为伯努利分布</strong></p>
<p>$$D (x;\theta_d)$$</p>
<p>$x$ 可以来自 $P_{data}$ 或者 生成器 $G$ </p>
<p><strong>高水平专家</strong></p>
<ul>
<li>if $x$ from $P_{data}$ ，Then $D(x)$  $\Uparrow$</li>
<li>if $x$ from $P_g$ ，Then $D(x)$ $\Downarrow$<ul>
<li>$D(x)$ 下降，即，$1-D(G(z))$ $\Uparrow$</li>
</ul>
</li>
</ul>
<p>为计算方便，可加上log</p>
<ul>
<li>if $x$ from $P_{data}$ ，Then $logD(x)$ $\Uparrow$</li>
<li>if $x$ from $P_g$ ，Then $log(1-D(G(z)))$ $\Uparrow$</li>
</ul>
<p>目标：</p>
<p>$$\max_D E_{x\sim{P_{data}}}[\log(D(x))]+E_{z\sim{P_{z}}}[log(1-D(G(z)))]$$</p>
<p><strong>高水平大师</strong></p>
<ul>
<li>if $x$ from $P_g$ ，Then $D(G(z))$ $\Uparrow$</li>
</ul>
<p>即</p>
<ul>
<li>if $x$ from $P_g$ ，Then $log(1-D(G(z)))$ $\Downarrow$</li>
</ul>
<p>目标：</p>
<p>$$\min_G E_{z\sim{P_{z}}}[log(1-D(G(z)))]$$</p>
<p>总目标：</p>
<p>$$\min_G \max_D E_{x\sim{P_{data}}}[\log(D(x))]+E_{z\sim{P_{z}}}[log(1-D(G(z)))]$$</p>
<h2 id="GAN全局最优解"><a href="#GAN全局最优解" class="headerlink" title="GAN全局最优解"></a>GAN全局最优解</h2><p>$\min_G \max_D E_{x\sim{P_{data}}}[\log(D(x))]+E_{z\sim{P_{z}}}[log(1-D(G(z)))]$</p>
<p>恒等</p>
<p>$V(D,G)&#x3D;\min_G \max_D E_{x\sim{P_{data}}}[\log(D(x))]+E_{x\sim{P_{g}}}[log(1-D(x)]$</p>
<p>for fix $G$ , $\max_D V(D,G)$</p>
<p>$\max_D V(D,G)&#x3D;\int{P_{data}\log{D}}dx+ \int{P_g\log(1-D)}dx$</p>
<p>$&#x3D;\int{[P_{data}\log{D}+ P_g\log(1-D)]}dx$</p>
<p>$&#x3D;\int{[P_{data}\log{D}+ P_g\log(1-D)]}dx$</p>
<p>对 $D$ 求偏导</p>
<p>$<br>\frac {\partial max_D V(D,G)} {\partial D } &#x3D; \int \frac {\partial} {\partial D } [P_{data}\log{D}+ P_g\log(1-D)]dx<br>$</p>
<p>$&#x3D;\int[ P_{data}{1 \over D}+P_g{-1 \over 1-D}]dx$</p>
<p>令导数为0：</p>
<p>$D^*<em>G &#x3D; {P</em>{data} \over P_{data}+P_g}$</p>
<p>故 $D$ 存在最大值</p>
<p>将 $D^*_G$ 带入 $V(D,G)$</p>
<p>$min_G max_D V(D,G)$</p>
<p>$&#x3D;min_G V(D^*_G,G)$</p>
<p>$&#x3D;\min_G E_{x\sim{P_{data}}}[\log{P_{data}\over P_{data}+P_g}]+E_{x\sim{P_{g}}}[log({P_{g}\over P_{data}+P_g})]$</p>
<p>$&#x3D;\min_G E_{x\sim{P_{data}}}[\log{1\over2}{P_{data}\over {P_{data}+P_g\over 2} }]+E_{x\sim{P_{g}}}[log({1\over2}{P_{g}\over {P_{data}+P_g\over2}})]$</p>
<p>$&#x3D;\min_G KL(P_{data} \parallel {P_{data}+P_g\over2})+KL(P_{g} \parallel {P_{data}+P_g\over2})-log4$</p>
<p>$\geqslant-log4$</p>
<p>当且仅当：$p_{data}&#x3D;P_g&#x3D;{P_{data}+P_g\over2}$ 时成立。</p>
<p><strong>最优解：</strong></p>
<p>$P^*<em>g &#x3D; P</em>{data}$</p>
<p>$D^*&#x3D;{1\over2}$</p>
<p>最终判别器在概率角度已经不能够分辨真假。</p>
<hr>
<ul>
<li>$P_{data}+P_g$ 这儿不是概率分布，凑出 $P_{data}+P_g \over 2$ 为概率分布。</li>
</ul>
<p>KL公式：</p>
<p>分子分母必须保证为概率分布</p>
<p>$$KL(p \parallel q) &#x3D;E_p log{p\over q}$$</p>
<p>打公式可太费劲了</p>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/03/19/tips/python_tips/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/03/19/tips/python_tips/" class="post-title-link" itemprop="url">python中的tips</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-03-19 02:51:55" itemprop="dateCreated datePublished" datetime="2020-03-19T02:51:55+08:00">2020-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%BC%96%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">编程</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Python中读写pkl文件"><a href="#Python中读写pkl文件" class="headerlink" title="Python中读写pkl文件"></a>Python中读写pkl文件</h2><ul>
<li><p>不采用压缩方式</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment">#write</span></span><br><span class="line">pack = <span class="string">&quot;xxx&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;demo.pkl&quot;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(pack,f)</span><br><span class="line"></span><br><span class="line"><span class="comment">#read</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;demo.pkl&quot;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = pickle.load(f)</span><br></pre></td></tr></table></figure>
</li>
<li><p>采用压缩方式</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bz2</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment">#write</span></span><br><span class="line">pack = <span class="string">&quot;xxx&quot;</span></span><br><span class="line"><span class="keyword">with</span> bz2.BZ2File(<span class="string">&quot;demo.pkl&quot;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(pack,f)</span><br><span class="line"></span><br><span class="line"><span class="comment">#read</span></span><br><span class="line"><span class="keyword">with</span> bz2.BZ2File(<span class="string">&quot;demo.pkl&quot;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = pickle.load(f)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>两种方式数据占用量差别在3-4倍，可以显著节省空间。</p>
<h2 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matplotlib</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> Image</span><br><span class="line">Image.imsave(<span class="string">&quot;save_path.jpg&quot;</span>, left_shoulder_rgb)</span><br></pre></td></tr></table></figure>
<p>保存值范围:0~1<br>保存格式为:RGB</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cv2</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">cv2.imwrite(<span class="string">&quot;save_path.jpg&quot;</span>,rgb,[<span class="built_in">int</span>(cv2.IMWRITE_JPEG_QUALITY),<span class="number">100</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>保存值范围:0~255<br>保存格式为:RBG</p>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><p>np.array 转成 tensor两种方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print(&quot;numpy&quot;,color.shape)</span></span><br><span class="line"><span class="comment"># numpy shape(240*320*3)</span></span><br><span class="line"><span class="comment"># cv2.imwrite(&quot;/home/sunshine/color&quot;+str(index)+&quot;.jpg&quot;,color,[int(cv2.IMWRITE_JPEG_QUALITY),100])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># to (3*240*320) </span></span><br><span class="line">color = trans.ToTensor()(color) </span><br><span class="line">sensor = trans.ToTensor()(sensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to (240*320*3)</span></span><br><span class="line"><span class="comment"># color = torch.from_numpy(color)</span></span><br><span class="line"><span class="comment"># sensor = torch.from_numpy(sensor)</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2021 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">[object Object]</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":500},"mobile":{"show":true},"react":{"opacity":0.5}});</script></body>
</html>
