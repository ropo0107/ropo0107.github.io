---
title: ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO)
categories: [å­¦æœ¯]
tags:   [RL, Note]
copyright: false
reward: false
rating: false
related_posts: false
date: 2020-05-29 11:51:55
---

# Policy Gradent

## Value-based RL vs Policy-based RL
- value base ä¸»è¦æ˜¯å­¦ä¹ ä¸€ä¸ªvalue function çš„è¿‘ä¼¼ï¼Œç„¶åé€šè¿‡è´ªå¿ƒçš„æ–¹æ³•å¾—åˆ°æœ€ä¼˜ç­–ç•¥
    $$a_t=\arg \max_aQ(a,st)$$
- policy base ç›´æ¥å­¦ä¹ ç­–ç•¥$\pi_\theta(a|s)$, å­¦ä¹ å‚æ•°$\theta$
    - Advantages:
        - better convergence properties: we are guaranteed to converge on a local optimum (worst case) or global optimum (best case)
        - Policy gradient is more effective in high-dimensional action space
        - Policy gradient can learn stochastic policies, while value function canâ€™t
    - Disadvantages:
        - å±€éƒ¨æœ€å¿§
        - æ–¹å·®å¤§
    - Deterministic: given a state, the policy returns a certain action to take
    - Stochastic:
        - probability distribution of distribute actions
        - a certain Gaussian distribution for continuous action
    
## Policy Gradent for Multi-step MDPs
- è½¨è¿¹åºåˆ—:
    $$\tau = (s_0, a_0, r_0, ... ,s_{T-1}, a_{T-1},r_{T-1},s_T, r_T) \sim (\pi_\theta, P(s_{t+1} | s_t, a_t))$$

- è½¨è¿¹æ¦‚ç‡:

    $$P(\tau;\theta) = \mu(s_0) \prod\limits_{t=0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$$

    $\pi_\theta(a_t|s_t)$ ä»…è¯¥é¡¹å’Œ Policy æœ‰å…³

    $p(s_{t+1}|s_t, a_t)$ å…¶ä¸ºåŠ¨åŠ›å­¦æ¨¡å‹ï¼Œ ä»…ä¸ç¯å¢ƒæœ‰å…³

- ç´¯è®¡å›æŠ¥:
    $$R(\tau) = \sum\limits_{t=0}^T r_t$$
- ä¼˜åŒ–ç›®æ ‡:
    $$J(\theta) = \sum\limits_\tau R(\tau) P(\tau;\theta)$$
    å¯¹è½¨è¿¹$\tau$è¿›è¡Œç´¯åŠ 
- Goal
  - goal:
    $$\theta^* = \arg\max\limits_\theta J(\theta) = \arg\max\limits_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$
  
  - gradient:
    
    $$\nabla_\theta Ja(\theta) =\nabla_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$
    
    $$=\sum\limits_\tau R(\tau)  P(\tau;\theta) \nabla_\theta logP(\tau;\theta)$$
  - Monte carlo:
    $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i=1}^m R(\tau_i) \nabla_\theta logP(\tau_i ;\theta)$$
    
    é‡‡æ ·åè½¨è¿¹æ¦‚ç‡ $P(\tau;\theta)$ å˜ä¸ºé‡‡æ ·çš„åŠ å’Œ
    
  - åˆ†è§£ $\nabla_\theta logP(\tau_i ;\theta)$:
  
    $$\nabla_\theta logP(\tau_i ;\theta) = \nabla_\theta log[\mu(s_0) \prod\limits_{t=0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)]$$
    $$=\nabla_\theta[log(\mu(s_0)) + \sum\limits_{t=0}^{T-1} (\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)))]$$
    $$=\sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$$
    
    $\theta$ä»…ä¸policyæœ‰å…³  
  - Finaly:
    $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i=1}^m [R(\tau_i) \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)]$$

    $\nabla_\theta \log\pi_\theta(a_t^i|s_t^i)$ä½œä¸º**score function**ä»£è¡¨æ¢¯åº¦æ›´æ–°æ–¹å‘

    $R(\tau_i)$è¡¨ç¤ºæ›´æ–°æ–¹å‘ä¸Šçš„æƒé‡ï¼Œ åå‘äºå¥½çš„è½¨è¿¹

## Comparison to Maximum Likelihood
- PG
    $$\nabla_\theta J(\theta) \approx \frac{1}{M} \sum\limits_{m=1}^M (\sum\limits_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m)) (\sum\limits_{t=1}^{T}r(s_t^m, a_t^m))$$
- Maximum Likelihood
    $$\nabla_\theta J_{ML}(\theta) \approx \frac{1}{M} \sum\limits_{m=1}^M (\sum\limits_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m))$$
- å¾€æ›´å¥½çš„è½¨è¿¹åç§»

    ![](/images/posts/rl/policy_gradent/pg_ml.png)

## Reduce Variance
- temporal causality
    - åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ± $G_t^i = \sum\limits_{t'=t}^{T-1}r_{t'}^i$
    - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i=1}^m \sum\limits_{t=0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$
    - 
    - REINFORCE:
        ![](/images/posts/rl/policy_gradent/reinforence.png)
    - n-step
        $$G_t^{(1)} = r_{t+1} + \gamma v(s_{t+1})$$
        $$G_t^{(2)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})$$
        $$G_t^{(\infty)} = r_{t+1} + \gamma r_{t+2} + ... +\gamma^{T-t-1}r_T$$
- add baseline
    - åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ± - å¹³å‡å¥–åŠ±ï¼Œè€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±$G_t^i -b^i(s_t)$
    - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i=1}^m \sum\limits_{t=0}^{T-1} (G_t^i-b^i(s_t)) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$
## PG Algorithm

![](/images/posts/rl/policy_gradent/pg_al.png)

# Problem of PG
- è®­ç»ƒä¸ç¨³å®š
    - Trust region (TRPO, PPO)
    - natural policy gradient (äºŒé˜¶ä¼˜åŒ–æ–¹æ³•)
- å¦‚ä½•ç¦»çº¿åŒ–è®­ç»ƒ **off-policy**
    - Importance samplingï¼ˆå°†åˆ†å¸ƒè½¬æ¢åˆ°å·²çŸ¥çš„åˆ†å¸ƒï¼‰

# Advantage Actor-Critic
- åˆ©ç”¨å½“å‰çŠ¶æ€çš„**Advantage function**ä½œä¸ºæƒé‡ ï¼Œè€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±
    ä¸€èˆ¬é€šè¿‡å‚æ•° $w$ æ¥æ‹Ÿåˆ
    $$A(s_t^i, a_t^i) = Q(s_t^i, a_t^i) - V(s_t^i)$$
- $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i=1}^m \sum\limits_{t=0}^{T-1} A_w(s_t^i,a_t^i) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$
- n step
  $$\hat{A}_t^{(1)} = r_{t+1} + \gamma v(s_{t+1})- v(s_t)$$
  $$\hat{A}_t^{(2)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})- v(s_t)$$
  $$\hat{A}_t^{(\infty)} = r_{t+1} + \gamma r_{t+2} + ... +\gamma^{T-t-1}r_T- v(s_t)$$
  $\hat{A}_t^{(1)}$ low variance ,high bias
  
  $\hat{A}_t^{(\infty)}$ high variance ,low bias

# TRPO
PG + off-policy + constrain
- æ­¥é•¿çš„é‡è¦æ€§
    - çŠ¶æ€å’Œå›æŠ¥åœ¨æ”¹å˜ç»Ÿè®¡ç‰¹æ€§, ä¼˜åŒ–è¿‡ç¨‹ä¸­å¾ˆéš¾ç¡®å®šæ›´æ–°æ­¥é•¿
    - policy ç»å¸¸ä¼šè¿‡æ—©é™·å…¥æ¬¡å¿§çš„å‡ ä¹ç¡®å®šçš„ç­–ç•¥ä¸­
    - TRPO é€šè¿‡ç½®ä¿¡åŸŸçš„æ–¹å¼ä¿è¯æ­¥é•¿ä¸ºå•è°ƒä¸å‡
    - ![](/images/posts/rl/policy_gradent/trust_region.png)
- ç›®æ ‡å‡½æ•°(å¸¦æœ‰æŠ˜æ‰£çš„å¥–åŠ±æœ€å¤§)ï¼š
    - $$\eta(\tilde{\pi}) =\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^{\infty} \gamma r(s_t)] $$
    - æ–°ç­–ç•¥ä¸‹çš„æœŸæœ›å›æŠ¥ = æ—§ç­–ç•¥ä¸‹çš„æœŸæœ›å›æŠ¥ + æ–°æ—§ç­–ç•¥æœŸæœ›å›æŠ¥çš„å·®
        $$\eta(\tilde{\pi}) = \eta(\pi) +\mathbb{E}_{\tau|\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_\pi(s_t,a_t)]$$

        $\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^\infty \gamma^t A_\pi(s_t,a_t)]$
        
        $=\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^\infty \gamma^t [Q_\pi(s_t,a_t)-V_\pi(s_t)]$

        $=\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^\infty \gamma^t [r(s_t)+ \gamma V_\pi(s_{t+1})-V_\pi(s_t)]$

        $=\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^\infty \gamma^t r(s_t)+ \sum\limits_{t=0}^\infty (\gamma V_\pi(s_{t+1})-V_\pi(s_t))]$

        $=\mathbb{E}_{\tau|\tilde{\pi}} [\sum\limits_{t=0}^\infty \gamma^t r(s_t)] + \mathbb{E}_{s_0}[-V_\pi(s_0)]$æ±‚å’Œå±•å¼€å‰åç›¸æ¶ˆ

        $= \eta(\tilde{\pi}) - \eta(\pi)$

- å¯¹äºä¸Šå¼å±•å¼€ 

    $$\eta(\tilde{\pi}) = \eta(\pi) +\mathbb{E}_{\tau|\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_\pi(s_t,a_t)]$$

    $$\rho_\pi(s) = P(s_0=s) + \gamma P(s_1=s) + \gamma^2P(s_2=s) + ... = \sum\limits_{t=0}^\infty \gamma^t P(s_t=s|\tilde{\pi} )$$

    $$\eta(\tilde{\pi}) = \eta(\pi) +\sum_{t=0}^\infty \sum\limits_{s} P(s_t = s|\tilde{\pi})\sum\limits_a \tilde{\pi}(a|s) \gamma^t A_\pi(s,a)$$

    $$\eta(\tilde{\pi}) = \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$
    è¿™æ˜¯çŠ¶æ€åˆ†å¸ƒç”±æ–°çš„ç­–ç•¥$\tilde{\pi}$ï¼Œ ä¸¥é‡ä¾èµ–æ–°çš„çŠ¶æ€åˆ†å¸ƒ

- two trick
    - å¿½ç•¥çŠ¶æ€åˆ†å¸ƒçš„å˜åŒ–ï¼Œ å‡å®š  
        $$\rho_{\tilde{\pi}}(s)\sim\rho_\pi(s)$$
    - é‡è¦æ€§é‡‡æ ·ï¼Œ
        - ![](/images/posts/rl/policy_gradent/importance.png)
        - $$\sum\limits_a \tilde{\pi_\theta}(a|s) A_{\theta_{old}}(s,a) = \mathbb{E}_{a\sim\pi_{old}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$
  
    - åŸç›®æ ‡ï¼š
        $$\eta(\tilde{\pi}) = \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$
    
    - æ–°ç›®æ ‡ï¼š
        $$L_{\pi_{\theta_{old}}} (\tilde{\pi})= \eta(\pi) + \mathbb{E}_{s\sim \rho_{\theta {old}}, a\sim\pi_{\theta {old}}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$
- æ¢¯åº¦æ›´æ–°æ–¹å‘ï¼Œ æ¢¯åº¦ä¼˜åŒ–æœ¬èº«å°±æ˜¯ä¸€é˜¶ä¼˜åŒ–ï¼Œ
    - åœ¨$\theta_{old}$ å¤„, ä¸€é˜¶å‡¸ä¼˜åŒ–æ–¹æ³•ä¸­ï¼Œå®Œå…¨ä¸€è‡´ï¼Œä¸€é˜¶å¯¼æ•°ä¸€è‡´ï¼Œ 
        $$\nabla_\theta \eta (\pi_{\theta_{old}}) = \nabla_\theta L_{\pi_{\theta_{old}}} (\pi_{\theta_{old}})$$ 
    - ![](/images/posts/rl/policy_gradent/one.png)

- æ¢¯åº¦æ›´æ–°æ­¥é•¿
    - $$\eta(\tilde{\pi}) \geqslant L_\pi(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})$$

        $where , C = \frac{2\epsilon\gamma}{(1-\gamma)^2}$

    - ä»¤ $M_{i}(\pi)  = L_{\pi_{i}}(\pi) - CD_{KL}^{MAX} (\pi_i, \pi)$
    
        $\eta (\pi_{i+1}) \geqslant M_i(\pi_{i+1})$
        
        ç­‰æ•ˆäºä¸Šé¢ä¸ç­‰å¼

        $\eta (\pi_{i}) = M_i(\pi_{i})$
        
        å°†$\pi_i$å¸¦å…¥ä¸ç­‰å¼ï¼Œä¸¤åˆ†å¸ƒç›¸åŒï¼Œ$D_{KL}$ä¸º0

        $\eta (\pi_{i+1}) - \eta (\pi_{i}) \geqslant M_i(\pi_{i+1}) -M_i(\pi_{i})$

        å³æœ€å¤§åŒ–$M_i$, å¯ä¿è¯æ›´æ–°æ­¥é•¿å•è°ƒä¸å‡
- Finally:
    - $\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})]$
    - $\max_\theta {mize} L_{\theta_{old}}(\theta)$

        subject to : $D_{KL}^{MAX} (\theta_{old}, \theta) \leqslant \delta$

        subject to : $\bar{D}_{KL}^{\rho_{\theta_{old}}} (\theta_{old}, \theta) \leqslant \delta$

    - çº¿æ€§åŒ–é€¼è¿‘ï¼Œ äºŒæ¬¡é€¼è¿‘å:
        
        $\max_\theta {mize}[\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta = \theta_{old}} \cdot (\theta-\theta_{old})]$gradent/ppo_clip.png)

        subject to : $\frac{1}{2} \left \| \theta- \theta_{old} \right \|^2 \leqslant \delta$

# PPO
æƒ³æ¯”äºTRPOçš„å˜åŒ–å°±æ˜¯

- TRPO ç”¨KLä½œä¸ºä¸€ä¸ªçº¦æŸï¼Œ è€Œ PPO åˆ™ä¸€å¿«å’Œ$\theta$è¿›è¡Œä¼˜åŒ–
- **Note:** KL ä¸ºè¾“å‡º$a$çš„è·ç¦»ï¼Œè€Œä¸æ˜¯å‚æ•°$\theta$ çš„è·ç¦»
- $\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) -\beta KL(\pi, \tilde{\pi})]$
    - If $ğ¾ğ¿(\theta, \theta_i) > KL_{max}$,  increase $\beta$
    - If $ğ¾ğ¿(\theta, \theta_i) < KL_{min}$,  decrease $\beta$
- ä¼˜åŒ–ç›®æ ‡ï¼š
    - $$L(\theta) = \mathbb{E}_{\tau \sim \pi_{old}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a) - \beta KL(\pi_{old}, \pi)]$$

    - $$L(\theta) = \mathbb{E}_{\tau \sim \pi_{old}}[min(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a), clip(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon, 1+\epsilon)A_{\theta_{old}}(s,a))]$$

        ![](/images/posts/rl/policy_gradent/ppo_clip.png)

# DDPG

- ç›®çš„ï¼š è®©DQNåº”ç”¨äºè¿ç»­çš„åŠ¨ä½œç©ºé—´
- 