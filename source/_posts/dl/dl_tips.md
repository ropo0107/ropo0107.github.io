---
title:  深度学习_Tips
categories: [学术]
tags:   [RL, Note]
copyright: false
reward: false
rating: false
related_posts: false
date: 2020-03-03 16:51:55
---


## 归纳偏置(Inductive bias)
[转载](https://www.zhihu.com/question/264264203/answer/830077823)

归纳偏置在机器学习中是一种很微妙的概念：在机器学习中，很多学习算法经常会对学习的问题做一些假设，这些假设就称为归纳偏置(Inductive Bias)。归纳偏置这个译名可能不能很好地帮助理解，不妨拆解开来看：归纳(Induction)是自然科学中常用的两大方法之一(归纳与演绎, induction and deduction)，指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；偏置(Bias)是指我们对模型的偏好。

因此，归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则(heuristics)，然后对模型做一定的约束，从而可以起到“模型选择”的作用，即从假设空间中选择出更符合现实规则的模型。其实，贝叶斯学习中的“先验(Prior)”这个叫法，可能比“归纳偏置”更直观一些。

归纳偏置在机器学习中几乎无处不可见。老生常谈的“奥卡姆剃刀”原理，即希望学习到的模型复杂度更低，就是一种归纳偏置。另外，还可以看见一些更强的一些假设：KNN中假设特征空间中相邻的样本倾向于属于同一类；SVM中假设好的分类器应该最大化类别边界距离；等等。

在深度学习方面也是一样。以神经网络为例，各式各样的网络结构/组件/机制往往就来源于归纳偏置。在卷积神经网络中，我们假设特征具有局部性(Locality)的特性，即当我们把相邻的一些特征放在一起，会更容易得到“解”；在循环神经网络中，我们假设每一时刻的计算依赖于历史计算结果；还有注意力机制，也是基于从人的直觉、生活经验归纳得到的规则。

在自然语言处理领域赫赫有名的word2vec，以及一些基于共现窗口的词嵌入方法，都是基于分布式假设：A word’s meaning is given by the words that frequently appear close-by. 这当然也可以看作是一种归纳偏置；一些自然语言理解的模型中加入解析树，也可以类似地理解。都是为了选择“更好”的模型。

## 正则项

正则化以及类似处理方法的本质（个人认为）是：这是一种对对象进行限制以满足某一特定目的的处理方法。

正则项的加入使原来的非适定问题变为适定问题。

     一个例子是，图像去噪的目标函数，如果只考虑一项（去噪后的图像的像素平均值和原图像一致），问题就有无穷多组解。但是，加上正则项（一般的是BV范数）以后，问题的解就变成唯一的了。

剩下的问题就是什么是适定问题？什么是不适定问题？

定解问题是适定的，是指它具备如下三个条件：

 - 问题的解存在；

 - 问题的解唯一；

 - 解连续依赖于定解条件，即定解条件（数据）改变很小时，相应的解也改变很小，也就是解具有稳定性。

 不满足上述三个条件之一的问题称为不适定(ill-posed)问题。特别地，对不满足第三个条件的定解问题，称为Hadamard意义下的不适定问题。


## pytorch 默认初始化网络参数

torch.nn.Conv2d 参数初始化

因为 Conv2d 继承 _ConvNd ,_ConvNd 有参数初始化。

## 特征学习两种范式
分别使用类标签和正负样本标签进行学习
- 类标签，一般需要用分类损失函数(比如:softmax + cross entropy)优化样本和权重向量之间的相识度。
- 样本对标签，用度量损失函数(比如:triplet loss)优化样本之间的相识度。
  
## BCELoss

cress_entropy_loss: 永不多分类问题一般输出层需要加sigmoid将值归一到（0-1）之间
同理 binary_cross_entropy_loss,也需要加入sigmoid进行归一化

