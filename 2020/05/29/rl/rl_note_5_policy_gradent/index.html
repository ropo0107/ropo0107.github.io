<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"ropo0107.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"slideLeftIn","sidebar":"slideDownBigOut"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Policy GradentValue-based RL vs Policy-based RL value base 主要是学习一个value function 的近似，然后通过贪心的方法得到最优策略  $$a_t&#x3D;\arg \max_aQ(a,st)$$ policy base 直接学习策略$\pi_\theta(a|s)$, 学习参数$\theta$ Advantages: bett">
<meta property="og:type" content="article">
<meta property="og:title" content="【rl_note】5. Policy Gradent(AC, TRPO, PPO)">
<meta property="og:url" content="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/index.html">
<meta property="og:site_name" content="无恙の博客">
<meta property="og:description" content="Policy GradentValue-based RL vs Policy-based RL value base 主要是学习一个value function 的近似，然后通过贪心的方法得到最优策略  $$a_t&#x3D;\arg \max_aQ(a,st)$$ policy base 直接学习策略$\pi_\theta(a|s)$, 学习参数$\theta$ Advantages: bett">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_ml.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/reinforence.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_al.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/trust_region.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/importance.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/one.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/ppo_clip.png">
<meta property="article:published_time" content="2020-05-29T03:51:55.000Z">
<meta property="article:modified_time" content="2025-01-02T14:55:25.973Z">
<meta property="article:author" content="无恙">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_ml.png">


<link rel="canonical" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/","path":"2020/05/29/rl/rl_note_5_policy_gradent/","title":"【rl_note】5. Policy Gradent(AC, TRPO, PPO)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>【rl_note】5. Policy Gradent(AC, TRPO, PPO) | 无恙の博客</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?3740b075e17748c0bde2d912b5d8f093"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">无恙の博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">幸得识卿桃花面，从此阡陌多暖春！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Policy-Gradent"><span class="nav-number">1.</span> <span class="nav-text">Policy Gradent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-based-RL-vs-Policy-based-RL"><span class="nav-number">1.1.</span> <span class="nav-text">Value-based RL vs Policy-based RL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Gradent-for-Multi-step-MDPs"><span class="nav-number">1.2.</span> <span class="nav-text">Policy Gradent for Multi-step MDPs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparison-to-Maximum-Likelihood"><span class="nav-number">1.3.</span> <span class="nav-text">Comparison to Maximum Likelihood</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reduce-Variance"><span class="nav-number">1.4.</span> <span class="nav-text">Reduce Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-causality-%E5%88%A9%E7%94%A8%E5%BD%93%E5%89%8D%E6%97%B6%E5%88%BB%E5%90%8E%E7%9A%84%E5%A5%96%E5%8A%B1%E8%80%8C%E4%B8%8D%E7%94%A8%E6%95%B4%E6%9D%A1%E8%BD%A8%E8%BF%B9%E7%9A%84%E5%A5%96%E5%8A%B1-G-t-i-sum-limits-t%E2%80%99-t-T-1-r-t%E2%80%99-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i"><span class="nav-number">1.5.</span> <span class="nav-text">temporal causality  - 利用当前时刻后的奖励而不用整条轨迹的奖励 $G_t^i &#x3D; \sum\limits_{t’&#x3D;t}^{T-1}r_{t’}^i$  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PG-Algorithm"><span class="nav-number">1.6.</span> <span class="nav-text">PG Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Problem-of-PG"><span class="nav-number">2.</span> <span class="nav-text">Problem of PG</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Advantage-Actor-Critic"><span class="nav-number">3.</span> <span class="nav-text">Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TRPO"><span class="nav-number">4.</span> <span class="nav-text">TRPO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PPO"><span class="nav-number">5.</span> <span class="nav-text">PPO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DDPG"><span class="nav-number">6.</span> <span class="nav-text">DDPG</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="无恙"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">无恙</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ropo0107" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ropo0107" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangzongtao123@gmail.com" title="E-Mail → mailto:wangzongtao123@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="/soup/darksoup/" title="&#x2F;soup&#x2F;darksoup&#x2F;">毒鸡汤</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="/soup/rainbow/" title="&#x2F;soup&#x2F;rainbow&#x2F;">彩虹屁</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="无恙">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无恙の博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="【rl_note】5. Policy Gradent(AC, TRPO, PPO) | 无恙の博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【rl_note】5. Policy Gradent(AC, TRPO, PPO)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">学术</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Policy-Gradent"><a href="#Policy-Gradent" class="headerlink" title="Policy Gradent"></a>Policy Gradent</h1><h2 id="Value-based-RL-vs-Policy-based-RL"><a href="#Value-based-RL-vs-Policy-based-RL" class="headerlink" title="Value-based RL vs Policy-based RL"></a>Value-based RL vs Policy-based RL</h2><ul>
<li>value base 主要是学习一个value function 的近似，然后通过贪心的方法得到最优策略<br>  $$a_t&#x3D;\arg \max_aQ(a,st)$$</li>
<li>policy base 直接学习策略$\pi_\theta(a|s)$, 学习参数$\theta$<ul>
<li>Advantages:<ul>
<li>better convergence properties: we are guaranteed to converge on a local optimum (worst case) or global optimum (best case)</li>
<li>Policy gradient is more effective in high-dimensional action space</li>
<li>Policy gradient can learn stochastic policies, while value function can’t</li>
</ul>
</li>
<li>Disadvantages:<ul>
<li>局部最忧</li>
<li>方差大</li>
</ul>
</li>
<li>Deterministic: given a state, the policy returns a certain action to take</li>
<li>Stochastic:<ul>
<li>probability distribution of distribute actions</li>
<li>a certain Gaussian distribution for continuous action</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Policy-Gradent-for-Multi-step-MDPs"><a href="#Policy-Gradent-for-Multi-step-MDPs" class="headerlink" title="Policy Gradent for Multi-step MDPs"></a>Policy Gradent for Multi-step MDPs</h2><ul>
<li><p>轨迹序列:<br>  $$\tau &#x3D; (s_0, a_0, r_0, … ,s_{T-1}, a_{T-1},r_{T-1},s_T, r_T) \sim (\pi_\theta, P(s_{t+1} | s_t, a_t))$$</p>
</li>
<li><p>轨迹概率:</p>
<p>  $$P(\tau;\theta) &#x3D; \mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$$</p>
<p>  $\pi_\theta(a_t|s_t)$ 仅该项和 Policy 有关</p>
<p>  $p(s_{t+1}|s_t, a_t)$ 其为动力学模型， 仅与环境有关</p>
</li>
<li><p>累计回报:<br>  $$R(\tau) &#x3D; \sum\limits_{t&#x3D;0}^T r_t$$</p>
</li>
<li><p>优化目标:<br>  $$J(\theta) &#x3D; \sum\limits_\tau R(\tau) P(\tau;\theta)$$<br>  对轨迹$\tau$进行累加</p>
</li>
<li><p>Goal</p>
<ul>
<li><p>goal:<br>$$\theta^* &#x3D; \arg\max\limits_\theta J(\theta) &#x3D; \arg\max\limits_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
</li>
<li><p>gradient:</p>
<p>$$\nabla_\theta Ja(\theta) &#x3D;\nabla_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
<p>$$&#x3D;\sum\limits_\tau R(\tau)  P(\tau;\theta) \nabla_\theta logP(\tau;\theta)$$</p>
</li>
<li><p>Monte carlo:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m R(\tau_i) \nabla_\theta logP(\tau_i ;\theta)$$</p>
<p>采样后轨迹概率 $P(\tau;\theta)$ 变为采样的加和</p>
</li>
<li><p>分解 $\nabla_\theta logP(\tau_i ;\theta)$:</p>
<p>$$\nabla_\theta logP(\tau_i ;\theta) &#x3D; \nabla_\theta log[\mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)]$$<br>$$&#x3D;\nabla_\theta[log(\mu(s_0)) + \sum\limits_{t&#x3D;0}^{T-1} (\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)))]$$<br>$$&#x3D;\sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$$</p>
<p>$\theta$仅与policy有关  </p>
</li>
<li><p>Finaly:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m [R(\tau_i) \sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)]$$</p>
<p>$\nabla_\theta \log\pi_\theta(a_t^i|s_t^i)$作为<strong>score function</strong>代表梯度更新方向</p>
<p>$R(\tau_i)$表示更新方向上的权重， 偏向于好的轨迹</p>
</li>
</ul>
</li>
</ul>
<h2 id="Comparison-to-Maximum-Likelihood"><a href="#Comparison-to-Maximum-Likelihood" class="headerlink" title="Comparison to Maximum Likelihood"></a>Comparison to Maximum Likelihood</h2><ul>
<li><p>PG<br>  $$\nabla_\theta J(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m)) (\sum\limits_{t&#x3D;1}^{T}r(s_t^m, a_t^m))$$</p>
</li>
<li><p>Maximum Likelihood<br>  $$\nabla_\theta J_{ML}(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m))$$</p>
</li>
<li><p>往更好的轨迹偏移</p>
<p>  <img src="/images/posts/rl/policy_gradent/pg_ml.png"></p>
</li>
</ul>
<h2 id="Reduce-Variance"><a href="#Reduce-Variance" class="headerlink" title="Reduce Variance"></a>Reduce Variance</h2><ul>
<li><h2 id="temporal-causality-利用当前时刻后的奖励而不用整条轨迹的奖励-G-t-i-sum-limits-t’-t-T-1-r-t’-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i"><a href="#temporal-causality-利用当前时刻后的奖励而不用整条轨迹的奖励-G-t-i-sum-limits-t’-t-T-1-r-t’-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i" class="headerlink" title="temporal causality  - 利用当前时刻后的奖励而不用整条轨迹的奖励 $G_t^i &#x3D; \sum\limits_{t’&#x3D;t}^{T-1}r_{t’}^i$  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$"></a>temporal causality<br>  - 利用当前时刻后的奖励而不用整条轨迹的奖励 $G_t^i &#x3D; \sum\limits_{t’&#x3D;t}^{T-1}r_{t’}^i$<br>  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</h2><ul>
<li>REINFORCE:<br>  <img src="/images/posts/rl/policy_gradent/reinforence.png"></li>
<li>n-step<br>  $$G_t^{(1)} &#x3D; r_{t+1} + \gamma v(s_{t+1})$$<br>  $$G_t^{(2)} &#x3D; r_{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})$$<br>  $$G_t^{(\infty)} &#x3D; r_{t+1} + \gamma r_{t+2} + … +\gamma^{T-t-1}r_T$$</li>
</ul>
</li>
<li>add baseline<ul>
<li>利用当前时刻后的奖励 - 平均奖励，而不用整条轨迹的奖励$G_t^i -b^i(s_t)$</li>
<li>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} (G_t^i-b^i(s_t)) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</li>
</ul>
</li>
</ul>
<h2 id="PG-Algorithm"><a href="#PG-Algorithm" class="headerlink" title="PG Algorithm"></a>PG Algorithm</h2><p><img src="/images/posts/rl/policy_gradent/pg_al.png"></p>
<h1 id="Problem-of-PG"><a href="#Problem-of-PG" class="headerlink" title="Problem of PG"></a>Problem of PG</h1><ul>
<li>训练不稳定<ul>
<li>Trust region (TRPO, PPO)</li>
<li>natural policy gradient (二阶优化方法)</li>
</ul>
</li>
<li>如何离线化训练 <strong>off-policy</strong><ul>
<li>Importance sampling（将分布转换到已知的分布）</li>
</ul>
</li>
</ul>
<h1 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h1><ul>
<li><p>利用当前状态的<strong>Advantage function</strong>作为权重 ，而不用整条轨迹的奖励<br>  一般通过参数 $w$ 来拟合<br>  $$A(s_t^i, a_t^i) &#x3D; Q(s_t^i, a_t^i) - V(s_t^i)$$</p>
</li>
<li><p>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} A_w(s_t^i,a_t^i) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</p>
</li>
<li><p>n step<br>$$\hat{A}<em>t^{(1)} &#x3D; r</em>{t+1} + \gamma v(s_{t+1})- v(s_t)$$<br>$$\hat{A}<em>t^{(2)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})- v(s_t)$$<br>$$\hat{A}<em>t^{(\infty)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + … +\gamma^{T-t-1}r_T- v(s_t)$$<br>$\hat{A}_t^{(1)}$ low variance ,high bias</p>
<p>$\hat{A}_t^{(\infty)}$ high variance ,low bias</p>
</li>
</ul>
<h1 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h1><p>PG + off-policy + constrain</p>
<ul>
<li><p>步长的重要性</p>
<ul>
<li>状态和回报在改变统计特性, 优化过程中很难确定更新步长</li>
<li>policy 经常会过早陷入次忧的几乎确定的策略中</li>
<li>TRPO 通过置信域的方式保证步长为单调不减</li>
<li><img src="/images/posts/rl/policy_gradent/trust_region.png"></li>
</ul>
</li>
<li><p>目标函数(带有折扣的奖励最大)：</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) &#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^{\infty} \gamma r(s_t)] $$</p>
</li>
<li><p>新策略下的期望回报 &#x3D; 旧策略下的期望回报 + 新旧策略期望回报的差<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [Q_\pi(s_t,a_t)-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [r(s_t)+ \gamma V_\pi(s_{t+1})-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)+ \sum\limits_{t&#x3D;0}^\infty (\gamma V_\pi(s_{t+1})-V_\pi(s_t))]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)] + \mathbb{E}<em>{s_0}[-V</em>\pi(s_0)]$求和展开前后相消</p>
<p>  $&#x3D; \eta(\tilde{\pi}) - \eta(\pi)$</p>
</li>
</ul>
</li>
<li><p>对于上式展开 </p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $$\rho_\pi(s) &#x3D; P(s_0&#x3D;s) + \gamma P(s_1&#x3D;s) + \gamma^2P(s_2&#x3D;s) + … &#x3D; \sum\limits_{t&#x3D;0}^\infty \gamma^t P(s_t&#x3D;s|\tilde{\pi} )$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum_{t&#x3D;0}^\infty \sum\limits_{s} P(s_t &#x3D; s|\tilde{\pi})\sum\limits_a \tilde{\pi}(a|s) \gamma^t A_\pi(s,a)$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$<br>  这是状态分布由新的策略$\tilde{\pi}$， 严重依赖新的状态分布</p>
</li>
<li><p>two trick</p>
<ul>
<li><p>忽略状态分布的变化， 假定<br>  $$\rho_{\tilde{\pi}}(s)\sim\rho_\pi(s)$$</p>
</li>
<li><p>重要性采样，</p>
<ul>
<li><img src="/images/posts/rl/policy_gradent/importance.png"></li>
<li>$$\sum\limits_a \tilde{\pi_\theta}(a|s) A_{\theta_{old}}(s,a) &#x3D; \mathbb{E}<em>{a\sim\pi</em>{old}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</li>
</ul>
</li>
<li><p>原目标：<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$</p>
</li>
<li><p>新目标：<br>  $$L_{\pi_{\theta_{old}}} (\tilde{\pi})&#x3D; \eta(\pi) + \mathbb{E}<em>{s\sim \rho</em>{\theta {old}}, a\sim\pi_{\theta {old}}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</p>
</li>
</ul>
</li>
<li><p>梯度更新方向， 梯度优化本身就是一阶优化，</p>
<ul>
<li>在$\theta_{old}$ 处, 一阶凸优化方法中，完全一致，一阶导数一致，<br>  $$\nabla_\theta \eta (\pi_{\theta_{old}}) &#x3D; \nabla_\theta L_{\pi_{\theta_{old}}} (\pi_{\theta_{old}})$$ </li>
<li><img src="/images/posts/rl/policy_gradent/one.png"></li>
</ul>
</li>
<li><p>梯度更新步长</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) \geqslant L_\pi(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})$$</p>
<p>  $where , C &#x3D; \frac{2\epsilon\gamma}{(1-\gamma)^2}$</p>
</li>
<li><p>令 $M_{i}(\pi)  &#x3D; L_{\pi_{i}}(\pi) - CD_{KL}^{MAX} (\pi_i, \pi)$</p>
<p>  $\eta (\pi_{i+1}) \geqslant M_i(\pi_{i+1})$</p>
<p>  等效于上面不等式</p>
<p>  $\eta (\pi_{i}) &#x3D; M_i(\pi_{i})$</p>
<p>  将$\pi_i$带入不等式，两分布相同，$D_{KL}$为0</p>
<p>  $\eta (\pi_{i+1}) - \eta (\pi_{i}) \geqslant M_i(\pi_{i+1}) -M_i(\pi_{i})$</p>
<p>  即最大化$M_i$, 可保证更新步长单调不减</p>
</li>
</ul>
</li>
<li><p>Finally:</p>
<ul>
<li><p>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})]$</p>
</li>
<li><p>$\max_\theta {mize} L_{\theta_{old}}(\theta)$</p>
<p>  subject to : $D_{KL}^{MAX} (\theta_{old}, \theta) \leqslant \delta$</p>
<p>  subject to : $\bar{D}<em>{KL}^{\rho</em>{\theta_{old}}} (\theta_{old}, \theta) \leqslant \delta$</p>
</li>
<li><p>线性化逼近， 二次逼近后:</p>
<p>  $\max_\theta {mize}[\nabla_\theta L_{\theta_{old}}(\theta)|<em>{\theta &#x3D; \theta</em>{old}} \cdot (\theta-\theta_{old})]$gradent&#x2F;ppo_clip.png)</p>
<p>  subject to : $\frac{1}{2} \left | \theta- \theta_{old} \right |^2 \leqslant \delta$</p>
</li>
</ul>
</li>
</ul>
<h1 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h1><p>想比于TRPO的变化就是</p>
<ul>
<li>TRPO 用KL作为一个约束， 而 PPO 则一快和$\theta$进行优化</li>
<li><strong>Note:</strong> KL 为输出$a$的距离，而不是参数$\theta$ 的距离</li>
<li>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) -\beta KL(\pi, \tilde{\pi})]$<ul>
<li>If $𝐾𝐿(\theta, \theta_i) &gt; KL_{max}$,  increase $\beta$</li>
<li>If $𝐾𝐿(\theta, \theta_i) &lt; KL_{min}$,  decrease $\beta$</li>
</ul>
</li>
<li>优化目标：<ul>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a) - \beta KL(\pi_{old}, \pi)]$$</p>
</li>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[min(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a), clip(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon, 1+\epsilon)A_{\theta_{old}}(s,a))]$$</p>
<p>  <img src="/images/posts/rl/policy_gradent/ppo_clip.png"></p>
</li>
</ul>
</li>
</ul>
<h1 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h1><ul>
<li>目的： 让DQN应用于连续的动作空间</li>
<li></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/RL/" rel="tag"># RL</a>
              <a href="/tags/Note/" rel="tag"># Note</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/04/24/dl/net_param_calculate/" rel="prev" title="【dl】 神经网络中参数量和计算量">
                  <i class="fa fa-angle-left"></i> 【dl】 神经网络中参数量和计算量
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/05/29/rl/rl_note_4_value_function_approximation/" rel="next" title="【rl_note】4. Value Function Approximation">
                  【rl_note】4. Value Function Approximation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2021 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">[object Object]</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":500},"mobile":{"show":true},"react":{"opacity":0.5}});</script></body>
</html>
