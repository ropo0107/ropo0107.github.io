<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"ropo0107.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"slideLeftIn","sidebar":"slideDownBigOut"}},"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Policy GradentValue-based RL vs Policy-based RL value base ä¸»è¦æ˜¯å­¦ä¹ ä¸€ä¸ªvalue function çš„è¿‘ä¼¼ï¼Œç„¶åé€šè¿‡è´ªå¿ƒçš„æ–¹æ³•å¾—åˆ°æœ€ä¼˜ç­–ç•¥  $$a_t&#x3D;\arg \max_aQ(a,st)$$ policy base ç›´æ¥å­¦ä¹ ç­–ç•¥$\pi_\theta(a|s)$, å­¦ä¹ å‚æ•°$\theta$ Advantages: bett">
<meta property="og:type" content="article">
<meta property="og:title" content="ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO)">
<meta property="og:url" content="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/index.html">
<meta property="og:site_name" content="æ— æ™ã®åšå®¢">
<meta property="og:description" content="Policy GradentValue-based RL vs Policy-based RL value base ä¸»è¦æ˜¯å­¦ä¹ ä¸€ä¸ªvalue function çš„è¿‘ä¼¼ï¼Œç„¶åé€šè¿‡è´ªå¿ƒçš„æ–¹æ³•å¾—åˆ°æœ€ä¼˜ç­–ç•¥  $$a_t&#x3D;\arg \max_aQ(a,st)$$ policy base ç›´æ¥å­¦ä¹ ç­–ç•¥$\pi_\theta(a|s)$, å­¦ä¹ å‚æ•°$\theta$ Advantages: bett">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_ml.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/reinforence.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_al.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/trust_region.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/importance.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/one.png">
<meta property="og:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/ppo_clip.png">
<meta property="article:published_time" content="2020-05-29T03:51:55.000Z">
<meta property="article:modified_time" content="2025-01-02T14:55:25.973Z">
<meta property="article:author" content="æ— æ™">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ropo0107.github.io/images/posts/rl/policy_gradent/pg_ml.png">


<link rel="canonical" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/","path":"2020/05/29/rl/rl_note_5_policy_gradent/","title":"ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO) | æ— æ™ã®åšå®¢</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?3740b075e17748c0bde2d912b5d8f093"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">æ— æ™ã®åšå®¢</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">å¹¸å¾—è¯†å¿æ¡ƒèŠ±é¢ï¼Œä»æ­¤é˜¡é™Œå¤šæš–æ˜¥ï¼</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Policy-Gradent"><span class="nav-number">1.</span> <span class="nav-text">Policy Gradent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-based-RL-vs-Policy-based-RL"><span class="nav-number">1.1.</span> <span class="nav-text">Value-based RL vs Policy-based RL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Gradent-for-Multi-step-MDPs"><span class="nav-number">1.2.</span> <span class="nav-text">Policy Gradent for Multi-step MDPs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparison-to-Maximum-Likelihood"><span class="nav-number">1.3.</span> <span class="nav-text">Comparison to Maximum Likelihood</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reduce-Variance"><span class="nav-number">1.4.</span> <span class="nav-text">Reduce Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-causality-%E5%88%A9%E7%94%A8%E5%BD%93%E5%89%8D%E6%97%B6%E5%88%BB%E5%90%8E%E7%9A%84%E5%A5%96%E5%8A%B1%E8%80%8C%E4%B8%8D%E7%94%A8%E6%95%B4%E6%9D%A1%E8%BD%A8%E8%BF%B9%E7%9A%84%E5%A5%96%E5%8A%B1-G-t-i-sum-limits-t%E2%80%99-t-T-1-r-t%E2%80%99-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i"><span class="nav-number">1.5.</span> <span class="nav-text">temporal causality  - åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ± $G_t^i &#x3D; \sum\limits_{tâ€™&#x3D;t}^{T-1}r_{tâ€™}^i$  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PG-Algorithm"><span class="nav-number">1.6.</span> <span class="nav-text">PG Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Problem-of-PG"><span class="nav-number">2.</span> <span class="nav-text">Problem of PG</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Advantage-Actor-Critic"><span class="nav-number">3.</span> <span class="nav-text">Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TRPO"><span class="nav-number">4.</span> <span class="nav-text">TRPO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PPO"><span class="nav-number">5.</span> <span class="nav-text">PPO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DDPG"><span class="nav-number">6.</span> <span class="nav-text">DDPG</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="æ— æ™"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">æ— æ™</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ropo0107" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;ropo0107" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangzongtao123@gmail.com" title="E-Mail â†’ mailto:wangzongtao123@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="è¿”å›é¡¶éƒ¨">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          é“¾æ¥
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="/soup/darksoup/" title="&#x2F;soup&#x2F;darksoup&#x2F;">æ¯’é¸¡æ±¤</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="/soup/rainbow/" title="&#x2F;soup&#x2F;rainbow&#x2F;">å½©è™¹å±</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ropo0107.github.io/2020/05/29/rl/rl_note_5_policy_gradent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="æ— æ™">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ— æ™ã®åšå®¢">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO) | æ— æ™ã®åšå®¢">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ã€rl_noteã€‘5. Policy Gradent(AC, TRPO, PPO)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2020-05-29 11:51:55" itemprop="dateCreated datePublished" datetime="2020-05-29T11:51:55+08:00">2020-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">æ›´æ–°äº</span>
      <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-01-02 22:55:25" itemprop="dateModified" datetime="2025-01-02T22:55:25+08:00">2025-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">å­¦æœ¯</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Policy-Gradent"><a href="#Policy-Gradent" class="headerlink" title="Policy Gradent"></a>Policy Gradent</h1><h2 id="Value-based-RL-vs-Policy-based-RL"><a href="#Value-based-RL-vs-Policy-based-RL" class="headerlink" title="Value-based RL vs Policy-based RL"></a>Value-based RL vs Policy-based RL</h2><ul>
<li>value base ä¸»è¦æ˜¯å­¦ä¹ ä¸€ä¸ªvalue function çš„è¿‘ä¼¼ï¼Œç„¶åé€šè¿‡è´ªå¿ƒçš„æ–¹æ³•å¾—åˆ°æœ€ä¼˜ç­–ç•¥<br>  $$a_t&#x3D;\arg \max_aQ(a,st)$$</li>
<li>policy base ç›´æ¥å­¦ä¹ ç­–ç•¥$\pi_\theta(a|s)$, å­¦ä¹ å‚æ•°$\theta$<ul>
<li>Advantages:<ul>
<li>better convergence properties: we are guaranteed to converge on a local optimum (worst case) or global optimum (best case)</li>
<li>Policy gradient is more effective in high-dimensional action space</li>
<li>Policy gradient can learn stochastic policies, while value function canâ€™t</li>
</ul>
</li>
<li>Disadvantages:<ul>
<li>å±€éƒ¨æœ€å¿§</li>
<li>æ–¹å·®å¤§</li>
</ul>
</li>
<li>Deterministic: given a state, the policy returns a certain action to take</li>
<li>Stochastic:<ul>
<li>probability distribution of distribute actions</li>
<li>a certain Gaussian distribution for continuous action</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Policy-Gradent-for-Multi-step-MDPs"><a href="#Policy-Gradent-for-Multi-step-MDPs" class="headerlink" title="Policy Gradent for Multi-step MDPs"></a>Policy Gradent for Multi-step MDPs</h2><ul>
<li><p>è½¨è¿¹åºåˆ—:<br>  $$\tau &#x3D; (s_0, a_0, r_0, â€¦ ,s_{T-1}, a_{T-1},r_{T-1},s_T, r_T) \sim (\pi_\theta, P(s_{t+1} | s_t, a_t))$$</p>
</li>
<li><p>è½¨è¿¹æ¦‚ç‡:</p>
<p>  $$P(\tau;\theta) &#x3D; \mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$$</p>
<p>  $\pi_\theta(a_t|s_t)$ ä»…è¯¥é¡¹å’Œ Policy æœ‰å…³</p>
<p>  $p(s_{t+1}|s_t, a_t)$ å…¶ä¸ºåŠ¨åŠ›å­¦æ¨¡å‹ï¼Œ ä»…ä¸ç¯å¢ƒæœ‰å…³</p>
</li>
<li><p>ç´¯è®¡å›æŠ¥:<br>  $$R(\tau) &#x3D; \sum\limits_{t&#x3D;0}^T r_t$$</p>
</li>
<li><p>ä¼˜åŒ–ç›®æ ‡:<br>  $$J(\theta) &#x3D; \sum\limits_\tau R(\tau) P(\tau;\theta)$$<br>  å¯¹è½¨è¿¹$\tau$è¿›è¡Œç´¯åŠ </p>
</li>
<li><p>Goal</p>
<ul>
<li><p>goal:<br>$$\theta^* &#x3D; \arg\max\limits_\theta J(\theta) &#x3D; \arg\max\limits_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
</li>
<li><p>gradient:</p>
<p>$$\nabla_\theta Ja(\theta) &#x3D;\nabla_\theta \sum\limits_\tau R(\tau) P(\tau;\theta)$$</p>
<p>$$&#x3D;\sum\limits_\tau R(\tau)  P(\tau;\theta) \nabla_\theta logP(\tau;\theta)$$</p>
</li>
<li><p>Monte carlo:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m R(\tau_i) \nabla_\theta logP(\tau_i ;\theta)$$</p>
<p>é‡‡æ ·åè½¨è¿¹æ¦‚ç‡ $P(\tau;\theta)$ å˜ä¸ºé‡‡æ ·çš„åŠ å’Œ</p>
</li>
<li><p>åˆ†è§£ $\nabla_\theta logP(\tau_i ;\theta)$:</p>
<p>$$\nabla_\theta logP(\tau_i ;\theta) &#x3D; \nabla_\theta log[\mu(s_0) \prod\limits_{t&#x3D;0}^{T-1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)]$$<br>$$&#x3D;\nabla_\theta[log(\mu(s_0)) + \sum\limits_{t&#x3D;0}^{T-1} (\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)))]$$<br>$$&#x3D;\sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$$</p>
<p>$\theta$ä»…ä¸policyæœ‰å…³  </p>
</li>
<li><p>Finaly:<br>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m [R(\tau_i) \sum\limits_{t&#x3D;0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)]$$</p>
<p>$\nabla_\theta \log\pi_\theta(a_t^i|s_t^i)$ä½œä¸º<strong>score function</strong>ä»£è¡¨æ¢¯åº¦æ›´æ–°æ–¹å‘</p>
<p>$R(\tau_i)$è¡¨ç¤ºæ›´æ–°æ–¹å‘ä¸Šçš„æƒé‡ï¼Œ åå‘äºå¥½çš„è½¨è¿¹</p>
</li>
</ul>
</li>
</ul>
<h2 id="Comparison-to-Maximum-Likelihood"><a href="#Comparison-to-Maximum-Likelihood" class="headerlink" title="Comparison to Maximum Likelihood"></a>Comparison to Maximum Likelihood</h2><ul>
<li><p>PG<br>  $$\nabla_\theta J(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m)) (\sum\limits_{t&#x3D;1}^{T}r(s_t^m, a_t^m))$$</p>
</li>
<li><p>Maximum Likelihood<br>  $$\nabla_\theta J_{ML}(\theta) \approx \frac{1}{M} \sum\limits_{m&#x3D;1}^M (\sum\limits_{t&#x3D;1}^{T} \nabla_\theta \log \pi_\theta(a_t^m|s_t^m))$$</p>
</li>
<li><p>å¾€æ›´å¥½çš„è½¨è¿¹åç§»</p>
<p>  <img src="/images/posts/rl/policy_gradent/pg_ml.png"></p>
</li>
</ul>
<h2 id="Reduce-Variance"><a href="#Reduce-Variance" class="headerlink" title="Reduce Variance"></a>Reduce Variance</h2><ul>
<li><h2 id="temporal-causality-åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±-G-t-i-sum-limits-tâ€™-t-T-1-r-tâ€™-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i"><a href="#temporal-causality-åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±-G-t-i-sum-limits-tâ€™-t-T-1-r-tâ€™-i-nabla-theta-J-theta-approx-frac-1-m-sum-limits-i-1-m-sum-limits-t-0-T-1-G-t-i-nabla-theta-log-pi-theta-a-t-i-s-t-i" class="headerlink" title="temporal causality  - åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ± $G_t^i &#x3D; \sum\limits_{tâ€™&#x3D;t}^{T-1}r_{tâ€™}^i$  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$"></a>temporal causality<br>  - åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ±è€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ± $G_t^i &#x3D; \sum\limits_{tâ€™&#x3D;t}^{T-1}r_{tâ€™}^i$<br>  - $$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} G_t^i \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</h2><ul>
<li>REINFORCE:<br>  <img src="/images/posts/rl/policy_gradent/reinforence.png"></li>
<li>n-step<br>  $$G_t^{(1)} &#x3D; r_{t+1} + \gamma v(s_{t+1})$$<br>  $$G_t^{(2)} &#x3D; r_{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})$$<br>  $$G_t^{(\infty)} &#x3D; r_{t+1} + \gamma r_{t+2} + â€¦ +\gamma^{T-t-1}r_T$$</li>
</ul>
</li>
<li>add baseline<ul>
<li>åˆ©ç”¨å½“å‰æ—¶åˆ»åçš„å¥–åŠ± - å¹³å‡å¥–åŠ±ï¼Œè€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±$G_t^i -b^i(s_t)$</li>
<li>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} (G_t^i-b^i(s_t)) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</li>
</ul>
</li>
</ul>
<h2 id="PG-Algorithm"><a href="#PG-Algorithm" class="headerlink" title="PG Algorithm"></a>PG Algorithm</h2><p><img src="/images/posts/rl/policy_gradent/pg_al.png"></p>
<h1 id="Problem-of-PG"><a href="#Problem-of-PG" class="headerlink" title="Problem of PG"></a>Problem of PG</h1><ul>
<li>è®­ç»ƒä¸ç¨³å®š<ul>
<li>Trust region (TRPO, PPO)</li>
<li>natural policy gradient (äºŒé˜¶ä¼˜åŒ–æ–¹æ³•)</li>
</ul>
</li>
<li>å¦‚ä½•ç¦»çº¿åŒ–è®­ç»ƒ <strong>off-policy</strong><ul>
<li>Importance samplingï¼ˆå°†åˆ†å¸ƒè½¬æ¢åˆ°å·²çŸ¥çš„åˆ†å¸ƒï¼‰</li>
</ul>
</li>
</ul>
<h1 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h1><ul>
<li><p>åˆ©ç”¨å½“å‰çŠ¶æ€çš„<strong>Advantage function</strong>ä½œä¸ºæƒé‡ ï¼Œè€Œä¸ç”¨æ•´æ¡è½¨è¿¹çš„å¥–åŠ±<br>  ä¸€èˆ¬é€šè¿‡å‚æ•° $w$ æ¥æ‹Ÿåˆ<br>  $$A(s_t^i, a_t^i) &#x3D; Q(s_t^i, a_t^i) - V(s_t^i)$$</p>
</li>
<li><p>$$\nabla_\theta J(\theta) \approx \frac{1}{m} \sum\limits_{i&#x3D;1}^m \sum\limits_{t&#x3D;0}^{T-1} A_w(s_t^i,a_t^i) \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)$$</p>
</li>
<li><p>n step<br>$$\hat{A}<em>t^{(1)} &#x3D; r</em>{t+1} + \gamma v(s_{t+1})- v(s_t)$$<br>$$\hat{A}<em>t^{(2)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + \gamma^2 v(s_{t+2})- v(s_t)$$<br>$$\hat{A}<em>t^{(\infty)} &#x3D; r</em>{t+1} + \gamma r_{t+2} + â€¦ +\gamma^{T-t-1}r_T- v(s_t)$$<br>$\hat{A}_t^{(1)}$ low variance ,high bias</p>
<p>$\hat{A}_t^{(\infty)}$ high variance ,low bias</p>
</li>
</ul>
<h1 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h1><p>PG + off-policy + constrain</p>
<ul>
<li><p>æ­¥é•¿çš„é‡è¦æ€§</p>
<ul>
<li>çŠ¶æ€å’Œå›æŠ¥åœ¨æ”¹å˜ç»Ÿè®¡ç‰¹æ€§, ä¼˜åŒ–è¿‡ç¨‹ä¸­å¾ˆéš¾ç¡®å®šæ›´æ–°æ­¥é•¿</li>
<li>policy ç»å¸¸ä¼šè¿‡æ—©é™·å…¥æ¬¡å¿§çš„å‡ ä¹ç¡®å®šçš„ç­–ç•¥ä¸­</li>
<li>TRPO é€šè¿‡ç½®ä¿¡åŸŸçš„æ–¹å¼ä¿è¯æ­¥é•¿ä¸ºå•è°ƒä¸å‡</li>
<li><img src="/images/posts/rl/policy_gradent/trust_region.png"></li>
</ul>
</li>
<li><p>ç›®æ ‡å‡½æ•°(å¸¦æœ‰æŠ˜æ‰£çš„å¥–åŠ±æœ€å¤§)ï¼š</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) &#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^{\infty} \gamma r(s_t)] $$</p>
</li>
<li><p>æ–°ç­–ç•¥ä¸‹çš„æœŸæœ›å›æŠ¥ &#x3D; æ—§ç­–ç•¥ä¸‹çš„æœŸæœ›å›æŠ¥ + æ–°æ—§ç­–ç•¥æœŸæœ›å›æŠ¥çš„å·®<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [Q_\pi(s_t,a_t)-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t [r(s_t)+ \gamma V_\pi(s_{t+1})-V_\pi(s_t)]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)+ \sum\limits_{t&#x3D;0}^\infty (\gamma V_\pi(s_{t+1})-V_\pi(s_t))]$</p>
<p>  $&#x3D;\mathbb{E}<em>{\tau|\tilde{\pi}} [\sum\limits</em>{t&#x3D;0}^\infty \gamma^t r(s_t)] + \mathbb{E}<em>{s_0}[-V</em>\pi(s_0)]$æ±‚å’Œå±•å¼€å‰åç›¸æ¶ˆ</p>
<p>  $&#x3D; \eta(\tilde{\pi}) - \eta(\pi)$</p>
</li>
</ul>
</li>
<li><p>å¯¹äºä¸Šå¼å±•å¼€ </p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\mathbb{E}<em>{\tau|\tilde{\pi}}[\sum\limits</em>{t&#x3D;0}^\infty \gamma^t A_\pi(s_t,a_t)]$$</p>
<p>  $$\rho_\pi(s) &#x3D; P(s_0&#x3D;s) + \gamma P(s_1&#x3D;s) + \gamma^2P(s_2&#x3D;s) + â€¦ &#x3D; \sum\limits_{t&#x3D;0}^\infty \gamma^t P(s_t&#x3D;s|\tilde{\pi} )$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum_{t&#x3D;0}^\infty \sum\limits_{s} P(s_t &#x3D; s|\tilde{\pi})\sum\limits_a \tilde{\pi}(a|s) \gamma^t A_\pi(s,a)$$</p>
<p>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$<br>  è¿™æ˜¯çŠ¶æ€åˆ†å¸ƒç”±æ–°çš„ç­–ç•¥$\tilde{\pi}$ï¼Œ ä¸¥é‡ä¾èµ–æ–°çš„çŠ¶æ€åˆ†å¸ƒ</p>
</li>
<li><p>two trick</p>
<ul>
<li><p>å¿½ç•¥çŠ¶æ€åˆ†å¸ƒçš„å˜åŒ–ï¼Œ å‡å®š<br>  $$\rho_{\tilde{\pi}}(s)\sim\rho_\pi(s)$$</p>
</li>
<li><p>é‡è¦æ€§é‡‡æ ·ï¼Œ</p>
<ul>
<li><img src="/images/posts/rl/policy_gradent/importance.png"></li>
<li>$$\sum\limits_a \tilde{\pi_\theta}(a|s) A_{\theta_{old}}(s,a) &#x3D; \mathbb{E}<em>{a\sim\pi</em>{old}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</li>
</ul>
</li>
<li><p>åŸç›®æ ‡ï¼š<br>  $$\eta(\tilde{\pi}) &#x3D; \eta(\pi) +\sum\limits_{s} \rho_{\tilde{\pi}}(s) \sum\limits_a \tilde{\pi}(a|s) A_\pi(s,a)$$</p>
</li>
<li><p>æ–°ç›®æ ‡ï¼š<br>  $$L_{\pi_{\theta_{old}}} (\tilde{\pi})&#x3D; \eta(\pi) + \mathbb{E}<em>{s\sim \rho</em>{\theta {old}}, a\sim\pi_{\theta {old}}} [\frac{\tilde{\pi_\theta}(a|s_n)}{\pi_{\theta_{old}}(a|s_n)}A_{\theta_{old}}(s,a)]$$</p>
</li>
</ul>
</li>
<li><p>æ¢¯åº¦æ›´æ–°æ–¹å‘ï¼Œ æ¢¯åº¦ä¼˜åŒ–æœ¬èº«å°±æ˜¯ä¸€é˜¶ä¼˜åŒ–ï¼Œ</p>
<ul>
<li>åœ¨$\theta_{old}$ å¤„, ä¸€é˜¶å‡¸ä¼˜åŒ–æ–¹æ³•ä¸­ï¼Œå®Œå…¨ä¸€è‡´ï¼Œä¸€é˜¶å¯¼æ•°ä¸€è‡´ï¼Œ<br>  $$\nabla_\theta \eta (\pi_{\theta_{old}}) &#x3D; \nabla_\theta L_{\pi_{\theta_{old}}} (\pi_{\theta_{old}})$$ </li>
<li><img src="/images/posts/rl/policy_gradent/one.png"></li>
</ul>
</li>
<li><p>æ¢¯åº¦æ›´æ–°æ­¥é•¿</p>
<ul>
<li><p>$$\eta(\tilde{\pi}) \geqslant L_\pi(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})$$</p>
<p>  $where , C &#x3D; \frac{2\epsilon\gamma}{(1-\gamma)^2}$</p>
</li>
<li><p>ä»¤ $M_{i}(\pi)  &#x3D; L_{\pi_{i}}(\pi) - CD_{KL}^{MAX} (\pi_i, \pi)$</p>
<p>  $\eta (\pi_{i+1}) \geqslant M_i(\pi_{i+1})$</p>
<p>  ç­‰æ•ˆäºä¸Šé¢ä¸ç­‰å¼</p>
<p>  $\eta (\pi_{i}) &#x3D; M_i(\pi_{i})$</p>
<p>  å°†$\pi_i$å¸¦å…¥ä¸ç­‰å¼ï¼Œä¸¤åˆ†å¸ƒç›¸åŒï¼Œ$D_{KL}$ä¸º0</p>
<p>  $\eta (\pi_{i+1}) - \eta (\pi_{i}) \geqslant M_i(\pi_{i+1}) -M_i(\pi_{i})$</p>
<p>  å³æœ€å¤§åŒ–$M_i$, å¯ä¿è¯æ›´æ–°æ­¥é•¿å•è°ƒä¸å‡</p>
</li>
</ul>
</li>
<li><p>Finally:</p>
<ul>
<li><p>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) - CD_{KL}^{MAX} (\pi, \tilde{\pi})]$</p>
</li>
<li><p>$\max_\theta {mize} L_{\theta_{old}}(\theta)$</p>
<p>  subject to : $D_{KL}^{MAX} (\theta_{old}, \theta) \leqslant \delta$</p>
<p>  subject to : $\bar{D}<em>{KL}^{\rho</em>{\theta_{old}}} (\theta_{old}, \theta) \leqslant \delta$</p>
</li>
<li><p>çº¿æ€§åŒ–é€¼è¿‘ï¼Œ äºŒæ¬¡é€¼è¿‘å:</p>
<p>  $\max_\theta {mize}[\nabla_\theta L_{\theta_{old}}(\theta)|<em>{\theta &#x3D; \theta</em>{old}} \cdot (\theta-\theta_{old})]$gradent&#x2F;ppo_clip.png)</p>
<p>  subject to : $\frac{1}{2} \left | \theta- \theta_{old} \right |^2 \leqslant \delta$</p>
</li>
</ul>
</li>
</ul>
<h1 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h1><p>æƒ³æ¯”äºTRPOçš„å˜åŒ–å°±æ˜¯</p>
<ul>
<li>TRPO ç”¨KLä½œä¸ºä¸€ä¸ªçº¦æŸï¼Œ è€Œ PPO åˆ™ä¸€å¿«å’Œ$\theta$è¿›è¡Œä¼˜åŒ–</li>
<li><strong>Note:</strong> KL ä¸ºè¾“å‡º$a$çš„è·ç¦»ï¼Œè€Œä¸æ˜¯å‚æ•°$\theta$ çš„è·ç¦»</li>
<li>$\max_\theta {mize}[ L_{\pi}(\tilde{\pi}) -\beta KL(\pi, \tilde{\pi})]$<ul>
<li>If $ğ¾ğ¿(\theta, \theta_i) &gt; KL_{max}$,  increase $\beta$</li>
<li>If $ğ¾ğ¿(\theta, \theta_i) &lt; KL_{min}$,  decrease $\beta$</li>
</ul>
</li>
<li>ä¼˜åŒ–ç›®æ ‡ï¼š<ul>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a) - \beta KL(\pi_{old}, \pi)]$$</p>
</li>
<li><p>$$L(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi</em>{old}}[min(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_{\theta_{old}}(s,a), clip(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},1-\epsilon, 1+\epsilon)A_{\theta_{old}}(s,a))]$$</p>
<p>  <img src="/images/posts/rl/policy_gradent/ppo_clip.png"></p>
</li>
</ul>
</li>
</ul>
<h1 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h1><ul>
<li>ç›®çš„ï¼š è®©DQNåº”ç”¨äºè¿ç»­çš„åŠ¨ä½œç©ºé—´</li>
<li></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/RL/" rel="tag"># RL</a>
              <a href="/tags/Note/" rel="tag"># Note</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/04/24/dl/net_param_calculate/" rel="prev" title="ã€dlã€‘ ç¥ç»ç½‘ç»œä¸­å‚æ•°é‡å’Œè®¡ç®—é‡">
                  <i class="fa fa-angle-left"></i> ã€dlã€‘ ç¥ç»ç½‘ç»œä¸­å‚æ•°é‡å’Œè®¡ç®—é‡
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/05/29/rl/rl_note_4_value_function_approximation/" rel="next" title="ã€rl_noteã€‘4. Value Function Approximation">
                  ã€rl_noteã€‘4. Value Function Approximation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2021 â€“ 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">[object Object]</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":500},"mobile":{"show":true},"react":{"opacity":0.5}});</script></body>
</html>
